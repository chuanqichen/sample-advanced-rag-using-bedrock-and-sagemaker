{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8617ba3-d126-49f3-a772-1a0ef066b3cf",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with Hierarchical chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88904b8a-3666-4f82-9801-712e9397b681",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "\n",
    "**Hierarchical Chunking**: Organizes your data into a hierarchical structure, allowing for more granular and efficient retrieval based on the inherent relationships within your data. \n",
    "\n",
    "Organizing your data into a hierarchical structure enables your **RAG (Retrieval-Augmented Generation)** workflow to efficiently navigate and retrieve information from complex, nested datasets. After documents are parsed, the first step is to **chunk** them based on the **parent** and **child chunking size**. \n",
    "\n",
    "- **Parent chunks (higher level)** represent larger segments, such as entire documents or sections.\n",
    "- **Child chunks (lower level)** represent smaller segments, such as paragraphs or sentences.\n",
    "\n",
    "The relationship between parent and child chunks is maintained, allowing for **efficient retrieval and navigation** of the corpus.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Efficient Retrieval**: The hierarchical structure enables faster and more targeted retrieval of relevant information by first performing a **semantic search** on child chunks and then returning the parent chunk. By replacing child chunks with parent chunks, we provide **larger and more comprehensive context** to the foundation model (FM).\n",
    "- **Context Preservation**: Organizing the corpus hierarchically helps maintain contextual relationships between chunks, ensuring more **coherent and contextually relevant** text generation.\n",
    "\n",
    "> **Note:** In hierarchical chunking, **parent chunks** are returned while **search is performed on child chunks**. As a result, you may see **fewer search results**, since one parent can have multiple child chunks.\n",
    "\n",
    "### **Best Use Cases**\n",
    "Hierarchical chunking is best suited for **complex documents** with a nested or hierarchical structure, such as:\n",
    "- **Technical manuals**\n",
    "- **Legal documents**\n",
    "- **Academic papers** with complex formatting and nested tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ee600b-1630-416e-acaa-d9de5630cb3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '307297743176',\n",
       " 'regionName': 'us-west-2',\n",
       " 'collectionArn': 'arn:aws:aoss:us-west-2:307297743176:collection/h7cmj732p9d3v91spkhd',\n",
       " 'collectionId': 'h7cmj732p9d3v91spkhd',\n",
       " 'vectorIndexName': 'ws-index-',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::307297743176:role/advanced-rag-workshop-bedrock_execution_role-us-west-2',\n",
       " 's3Bucket': '307297743176-us-west-2-advanced-rag-workshop',\n",
       " 'kbFixedChunk': '4P6PBDDEGL',\n",
       " 'kbSemanticChunk': 'IC3ZCBORXT',\n",
       " 'kbCustomChunk': 'Q2T9CZ5VFA'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e5afe-e197-458a-af71-6b5fc004d491",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05bdd0f-806c-44f8-9840-ba955e5c43ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from retrying import retry\n",
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock agent client with the appropriate region\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Helper function to create a knowledge base with retry mechanism\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=3)\n",
    "def create_knowledge_base_func(name, description, chunking_type):\n",
    "    \"\"\"\n",
    "    Creates a knowledge base in Amazon Bedrock with OpenSearch Serverless storage configuration.\n",
    "    \n",
    "    Args:\n",
    "    - name (str): The name of the knowledge base.\n",
    "    - description (str): A brief description of the knowledge base.\n",
    "    - chunking_type (str): The type of chunking to be used (e.g., 'fixed', 'hierarchical').\n",
    "    \n",
    "    Returns:\n",
    "    - dict: The knowledge base details returned by the API call.\n",
    "    \"\"\"\n",
    "    # Define the embedding model ARN used by Bedrock for document embedding\n",
    "    embedding_model_arn = f\"arn:aws:bedrock:{variables['regionName']}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "    # Define OpenSearch Serverless configuration\n",
    "    opensearch_serverless_configuration = {\n",
    "        \"collectionArn\": variables[\"collectionArn\"],  # ARN of the OpenSearch collection\n",
    "        \"vectorIndexName\": variables[\"vectorIndexName\"] + chunking_type,  # Vector index name based on chunking type\n",
    "        \"fieldMapping\": {  # Field mapping for the OpenSearch index\n",
    "            \"vectorField\": \"vector\",\n",
    "            \"textField\": \"text\",\n",
    "            \"metadataField\": \"text-metadata\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print the OpenSearch configuration for debugging purposes\n",
    "    print(opensearch_serverless_configuration)\n",
    "    \n",
    "    try:\n",
    "        # Call the Bedrock agent's create_knowledge_base API to create the knowledge base\n",
    "        create_kb_response = bedrock_agent.create_knowledge_base(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            roleArn=variables[\"bedrockExecutionRoleArn\"],  # ARN of the execution role\n",
    "            knowledgeBaseConfiguration={\n",
    "                \"type\": \"VECTOR\",  # Define the knowledge base as a vector knowledge base\n",
    "                \"vectorKnowledgeBaseConfiguration\": {\n",
    "                    \"embeddingModelArn\": embedding_model_arn  # Define the embedding model ARN\n",
    "                }\n",
    "            },\n",
    "            storageConfiguration={\n",
    "                \"type\": \"OPENSEARCH_SERVERLESS\",  # Using OpenSearch Serverless for storage\n",
    "                \"opensearchServerlessConfiguration\": opensearch_serverless_configuration\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Return the knowledge base details from the response\n",
    "        return create_kb_response[\"knowledgeBase\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle exceptions (e.g., API failures) and print the error message\n",
    "        print(f\"Error creating knowledge base: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469dcd35-6fa9-4d06-a436-78b2136e269f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collectionArn': 'arn:aws:aoss:us-west-2:307297743176:collection/h7cmj732p9d3v91spkhd', 'vectorIndexName': 'ws-index-hierarchical', 'fieldMapping': {'vectorField': 'vector', 'textField': 'text', 'metadataField': 'text-metadata'}}\n",
      "OpenSearch Knowledge Response: {\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"5c82629d-651b-4096-8ab6-ed359ad6efac\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"date\": \"Mon, 07 Apr 2025 15:44:39 GMT\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"content-length\": \"970\",\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"x-amzn-requestid\": \"5c82629d-651b-4096-8ab6-ed359ad6efac\",\n",
      "            \"x-amz-apigw-id\": \"IqLkSEBSvHcEfIA=\",\n",
      "            \"x-amzn-trace-id\": \"Root=1-67f3f2e7-7d7c7c056cfd69de7c7dd246\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"knowledgeBase\": {\n",
      "        \"createdAt\": \"2025-04-07 15:44:39.485106+00:00\",\n",
      "        \"description\": \"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
      "        \"knowledgeBaseArn\": \"arn:aws:bedrock:us-west-2:307297743176:knowledge-base/1YIFVW0Z5E\",\n",
      "        \"knowledgeBaseConfiguration\": {\n",
      "            \"type\": \"VECTOR\",\n",
      "            \"vectorKnowledgeBaseConfiguration\": {\n",
      "                \"embeddingModelArn\": \"arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v2:0\"\n",
      "            }\n",
      "        },\n",
      "        \"knowledgeBaseId\": \"1YIFVW0Z5E\",\n",
      "        \"name\": \"advanced-rag-workshop-hierarchical-chunking\",\n",
      "        \"roleArn\": \"arn:aws:iam::307297743176:role/advanced-rag-workshop-bedrock_execution_role-us-west-2\",\n",
      "        \"status\": \"CREATING\",\n",
      "        \"storageConfiguration\": {\n",
      "            \"opensearchServerlessConfiguration\": {\n",
      "                \"collectionArn\": \"arn:aws:aoss:us-west-2:307297743176:collection/h7cmj732p9d3v91spkhd\",\n",
      "                \"fieldMapping\": {\n",
      "                    \"metadataField\": \"text-metadata\",\n",
      "                    \"textField\": \"text\",\n",
      "                    \"vectorField\": \"vector\"\n",
      "                },\n",
      "                \"vectorIndexName\": \"ws-index-hierarchical\"\n",
      "            },\n",
      "            \"type\": \"OPENSEARCH_SERVERLESS\"\n",
      "        },\n",
      "        \"updatedAt\": \"2025-04-07 15:44:39.485106+00:00\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Try to create a knowledge base, but handle the case where it returns None\n",
    "kb = create_knowledge_base_func(\n",
    "    name=\"advanced-rag-workshop-hierarchical-chunking\",\n",
    "    description=\"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
    "    chunking_type=\"hierarchical\"\n",
    ")\n",
    "\n",
    "# Check if kb is None (meaning creation failed)\n",
    "if kb is None:\n",
    "    print(\"Knowledge Base creation returned None. Checking if it already exists...\")\n",
    "    \n",
    "    # List all knowledge bases to find the one that already exists\n",
    "    list_kb_response = bedrock_agent.list_knowledge_bases()\n",
    "    \n",
    "    # Look for a knowledge base with the desired name\n",
    "    for existing_kb in list_kb_response.get('knowledgeBaseSummaries', []):\n",
    "        if existing_kb['name'] == \"advanced-rag-workshop-hierarchical-chunking\":\n",
    "            kb_id = existing_kb['knowledgeBaseId']\n",
    "            print(f\"Found existing knowledge base with ID: {kb_id}\")\n",
    "            \n",
    "            # Get the details of the existing knowledge base\n",
    "            get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "            \n",
    "            # Read existing variables to preserve other fields\n",
    "            try:\n",
    "                # Read existing variables\n",
    "                with open(\"variables.json\", \"r\") as f:\n",
    "                    existing_variables = json.load(f)\n",
    "            except (FileNotFoundError, json.JSONDecodeError):\n",
    "                # If file doesn't exist or is invalid JSON\n",
    "                existing_variables = {}\n",
    "            \n",
    "            # Update only the hierarchical chunking value\n",
    "            existing_variables[\"kbHierarchicalChunk\"] = kb_id\n",
    "                            \n",
    "            # Write back all variables\n",
    "            with open(\"variables.json\", \"w\") as f:\n",
    "                json.dump(existing_variables, f, indent=4, default=str)\n",
    "            \n",
    "            # Print the retrieved knowledge base response\n",
    "            print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "            break        \n",
    "    else:\n",
    "        print(\"Could not find a knowledge base with the specified name.\")\n",
    "else:\n",
    "    # KB was created successfully, proceed with original flow\n",
    "    try:\n",
    "        # Retrieve details of the newly created knowledge base\n",
    "        get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "\n",
    "        # Read existing variables to preserve other fields\n",
    "        try:\n",
    "            with open(\"variables.json\", \"r\") as f:\n",
    "                variables = json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            pass  # Use existing variables dict\n",
    "\n",
    "        # Update the variables dictionary with the new knowledge base ID\n",
    "        variables[\"kbHierarchicalChunk\"] = kb['knowledgeBaseId']\n",
    "\n",
    "        # Save updated variables to a JSON file, handling datetime serialization\n",
    "        with open(\"variables.json\", \"w\") as f:\n",
    "            json.dump(variables, f, indent=4, default=str)\n",
    "\n",
    "        # Print the retrieved knowledge base response in a readable format\n",
    "        print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing newly created knowledge base: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ec04a-b09a-40dc-8309-88dee13051a3",
   "metadata": {},
   "source": [
    "### 2. Create Datasources for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2878b51c-6295-484c-a8d0-7be85c23d3f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving knowledge base ID for hierarchical chunking...\n",
      "Found existing knowledge base with ID: 1YIFVW0Z5E\n",
      "Creating new data source 'advanced-rag-example' with hierarchical chunking...\n",
      "Hierarchical chunking data source created successfully.\n",
      "{'createdAt': datetime.datetime(2025, 4, 7, 15, 44, 43, 545635, tzinfo=tzlocal()), 'dataDeletionPolicy': 'DELETE', 'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::307297743176-us-west-2-advanced-rag-workshop', 'inclusionPrefixes': ['data']}, 'type': 'S3'}, 'dataSourceId': 'FMNOCM7JWH', 'description': 'A data source for Advanced RAG workshop', 'knowledgeBaseId': '1YIFVW0Z5E', 'name': 'advanced-rag-example', 'status': 'AVAILABLE', 'updatedAt': datetime.datetime(2025, 4, 7, 15, 44, 43, 545635, tzinfo=tzlocal()), 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'HIERARCHICAL', 'hierarchicalChunkingConfiguration': {'levelConfigurations': [{'maxTokens': 1500}, {'maxTokens': 300}], 'overlapTokens': 60}}}}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# First, retrieve the knowledge base ID by listing all KBs and finding the hierarchical one\n",
    "print(\"Retrieving knowledge base ID for hierarchical chunking...\")\n",
    "list_kb_response = bedrock_agent.list_knowledge_bases()\n",
    "kb_id = None\n",
    "\n",
    "# Look for the hierarchical chunking knowledge base by name\n",
    "for existing_kb in list_kb_response.get('knowledgeBaseSummaries', []):\n",
    "    if existing_kb['name'] == \"advanced-rag-workshop-hierarchical-chunking\":\n",
    "        kb_id = existing_kb['knowledgeBaseId']\n",
    "        print(f\"Found existing knowledge base with ID: {kb_id}\")\n",
    "        \n",
    "        # Read existing variables to preserve other fields\n",
    "        try:\n",
    "            with open(\"variables.json\", \"r\") as f:\n",
    "                variables = json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            pass\n",
    "            \n",
    "        # Update variables with the hierarchical KB ID (if needed)\n",
    "        variables[\"kbHierarchicalChunk\"] = kb_id\n",
    "        \n",
    "        # Save updated variables\n",
    "        with open(\"variables.json\", \"w\") as f:\n",
    "            json.dump(variables, f, indent=4, default=str)\n",
    "            \n",
    "        break\n",
    "else:\n",
    "    print(\"Could not find the hierarchical chunking knowledge base.\")\n",
    "\n",
    "# Proceed only if we found the knowledge base ID\n",
    "if kb_id:\n",
    "    # Define the chunking strategy for ingestion using a hierarchical approach\n",
    "    chunking_strategy_configuration = {\n",
    "        \"chunkingStrategy\": \"HIERARCHICAL\",\n",
    "        \"hierarchicalChunkingConfiguration\": {\n",
    "            \"levelConfigurations\": [\n",
    "                {\"maxTokens\": 1500},\n",
    "                {\"maxTokens\": 300}\n",
    "            ],\n",
    "            \"overlapTokens\": 60\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # The data source to ingest documents from, with the data prefix\n",
    "    s3_configuration = {\n",
    "        \"bucketArn\": f\"arn:aws:s3:::{variables['s3Bucket']}\",\n",
    "        \"inclusionPrefixes\": [\"data\"]  # Only include objects with the \"data\" prefix\n",
    "    }\n",
    "\n",
    "    data_source_name = \"advanced-rag-example\"\n",
    "\n",
    "    # First, check if a data source with this name already exists in Bedrock\n",
    "    try:\n",
    "        # List all data sources for the knowledge base\n",
    "        list_ds_response = bedrock_agent.list_data_sources(\n",
    "            knowledgeBaseId=kb_id\n",
    "        )\n",
    "        \n",
    "        # Check if our named data source exists\n",
    "        existing_ds = None\n",
    "        for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "            if ds['name'] == data_source_name:\n",
    "                existing_ds = ds\n",
    "                break\n",
    "        \n",
    "        # If it exists, delete it\n",
    "        if existing_ds:\n",
    "            print(f\"Found existing data source '{data_source_name}'. Deleting it...\")\n",
    "            bedrock_agent.delete_data_source(\n",
    "                knowledgeBaseId=kb_id,\n",
    "                dataSourceId=existing_ds[\"dataSourceId\"]\n",
    "            )\n",
    "            print(\"Waiting for data source deletion to complete...\")\n",
    "            time.sleep(10)\n",
    "            print(\"Data source deleted successfully.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error while checking or deleting data source: {e}\")\n",
    "\n",
    "    # Now create a new data source\n",
    "    try:\n",
    "        print(f\"Creating new data source '{data_source_name}' with hierarchical chunking...\")\n",
    "        create_ds_response = bedrock_agent.create_data_source(\n",
    "            name=data_source_name,\n",
    "            description=\"A data source for Advanced RAG workshop\",\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceConfiguration={\n",
    "                \"type\": \"S3\",\n",
    "                \"s3Configuration\": s3_configuration\n",
    "            },\n",
    "            vectorIngestionConfiguration={\n",
    "                \"chunkingConfiguration\": chunking_strategy_configuration\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Store the created data source object\n",
    "        ds_hierarchical_chunk = create_ds_response[\"dataSource\"]\n",
    "        print(f\"Hierarchical chunking data source created successfully.\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ConflictException':\n",
    "            print(f\"Data source '{data_source_name}' still exists. Retrieving it...\")\n",
    "            # Get the existing data source\n",
    "            list_ds_response = bedrock_agent.list_data_sources(\n",
    "                knowledgeBaseId=kb_id\n",
    "            )\n",
    "            for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "                if ds['name'] == data_source_name:\n",
    "                    ds_hierarchical_chunk = ds\n",
    "                    print(f\"Retrieved existing data source: {ds['dataSourceId']}\")\n",
    "                    break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Print the data source information\n",
    "    print(ds_hierarchical_chunk)\n",
    "else:\n",
    "    print(\"Cannot proceed without a valid knowledge base ID.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946a50d-0255-41d0-bb9f-89dc4d4f9ead",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c50da",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in the OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9353d724-c272-4787-a487-80c2b1fa3a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion job started successfully\n",
      "\n",
      "running...\n",
      "running...\n",
      "running...\n",
      "running...\n",
      "running...\n",
      "running...\n",
      "running...\n",
      "Job completed successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# Get the knowledge base ID from variables.json\n",
    "try:\n",
    "    with open(\"variables.json\", \"r\") as f:\n",
    "        variables = json.load(f)\n",
    "    kb_id = variables.get(\"kbHierarchicalChunk\")\n",
    "    \n",
    "    if not kb_id:\n",
    "        print(\"Knowledge base ID not found in variables.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading knowledge base ID: {e}\")\n",
    "    kb_id = None\n",
    "\n",
    "# Start an ingestion job for the given data source and knowledge base\n",
    "try:\n",
    "    # Initiate the ingestion job and capture the response\n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb_id,  # Use the retrieved knowledge base ID instead of kb['knowledgeBaseId']\n",
    "        dataSourceId=ds_hierarchical_chunk[\"dataSourceId\"]\n",
    "    )\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(f\"Ingestion job started successfully\\n\")\n",
    "\n",
    "    # Monitor the ingestion job status until it completes\n",
    "    while job['status'] != 'COMPLETE':\n",
    "        print(\"running...\")\n",
    "        time.sleep(10)\n",
    "        # Check the status of the ingestion job\n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id,  # Use the retrieved knowledge base ID here too\n",
    "            dataSourceId=ds_hierarchical_chunk[\"dataSourceId\"],\n",
    "            ingestionJobId=job[\"ingestionJobId\"]\n",
    "        )\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "\n",
    "    print(f\"Job completed successfully\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't start job.\\n\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fd3ab-cc8b-4a24-9696-4ac1190ca0bd",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cc7a7f-773b-47b2-aa47-bbbe22a2e2c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Like in the Ambiguous SELECT Column case, after generating the user clarification we filter the data based on some rules. We discard those samples that contain the new column or miss the removed column in the clarification. We finally re- fine the conversation, execute the clarification SQL to get the results, and generate the natural language explanation of the results.     E.2 Ambiguous Values Within Column     In stage 1, we extract the values appearing in the Where clause of the SQLs of all questions in the Spider dataset. We prompt the LLM to generate values that are similar but not equivalent to each other. We then construct the Ambiguous Values Within Column ambiguous data by constructing a new schema from the original schema by remov- ing the original value and adding the newly gener- ated ambiguous values. For example, for the value \\u201cchemistry\\u201d the LLM generates two ambiguous val- ues, \\u201corganic chemistry\\u201d and \\u201cphysical chemistry\\u201d. In stage 2, we construct the assistant\\u2019s helpful re- sponse by using a template that points out the pres- ence of two Ambiguous SELECT Columns. We generate the assistant\\u2019s clarified SQL by replacing the original value with one of the ambiguous values. We then provide this data in the form of a conver- sation as input to the LLM and prompt it to come up with the user clarification response. We then discard those data where the user clarification does not mention the newly generated values. Finally, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.3 Ambiguous Filter Criteria     To construct the ambiguous filter criteria data, we utilized the SPIDER dataset. Instead of modify-     ing the databases, we prompted a Large Language Model (LLM) to modify the user questions to in- troduce ambiguity. Specifically, we employed the following techniques: 1. Replacing specific fil- ter values with relative terms like \\\"little/large,\\\" \\\"young/old,\\\" \\\"slow/fast,\\\" etc. 2. Using descriptive terms instead of explicitly stating the original filter value. The modified questions resembled those in the BIRD dataset that require additional \\\"evidence\\\" or definitions to convert text to SQL. After mod- ifying the questions, we prompted the LLM with different instructions to generate a response from the database assistant\\u2019s perspective, explaining why the question was ambiguous. Finally, we used the original (unmodified) user question as the clarified follow-up question, and the corresponding SQL as the gold SQL after the user\\u2019s clarification.     E.4 Nonexistent SELECT Column     In stage 1, we extract the columns appearing in the Select clause of the SQLs of all questions in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\\u2019s helpful response using a template that states that the column required for answering the question is missing from the schema. We construct the final SQL by replacing the missing column from the schema (in the Select clause) with a col- umn that exists in the schema. We convert this data into conversational data and prompt the model to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.5 Nonexistent Filter Value     In stage 1, we extract the values appearing in the Where clause of the SQLs of all questions in the Spider dataset. For constructing the problematic data we construct a new schema by removing the values required for answering the question from the schema. In stage 2, we construct the assistant\\u2019s helpful response using a template that mentions that the value mentioned in the question is not present in the schema. We construct the clarification SQL by replacing the removed value with another value present in the schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the naturallanguage explanation of the results.     E.6 Unsupported Join     In stage 1, to construct the problematic data, we consider the unique schemas of the Spider dataset and prompt the LLM to generate a new schema with at least two new tables and corresponding columns such that the new tables have a foreign key relationship with themselves but not with any other column in the schema. For example, for a schema containing student information like student grade, teacher details, etc. the LLM produces two new tables of library and books that have a foreign key relationship with each other but not with any other table in the original schema. In stage 2, we construct the assistant\\u2019s helpful response using a template stating that the question requires joining tables of the schema that have no relationship with each other. We construct a clarification SQL by using SQL from the Spider dataset corresponding to the original schema. We then convert this data into conversational data and prompt the LLM to generate the user clarification response. In stage 3, we refine the conversation, execute the clarifica- tion SQL get the results, and generate the natural language explanation of the results.     E.7 Nonexistent WHERE Column     In stage 1, we extract the columns present in the Where clause of the SQLs in the Spider dataset and construct new schemas by removing the columns required for answering the respective questions. In stage 2, we construct the assistant\\u2019s helpful re- sponse using a template that mentions that the in- formation required for answering the question is not present in the schema. We construct the clar- ification SQL by finding a SQL from the Spider dataset whose Select columns match the problem- atic question and whose Where columns are present in the schema. We convert this data into a conver- sational format and prompt the LLM to generate the user clarification response. In stage 3, we re- fine the conversation, execute the clarification SQL get the results, and generate the natural language explanation of the results.     F Experimental Settings     We use Anthropic AI\\u2019s Claude 3 Sonnet via Ama- zon Bedrock 9 for all our data generation. For the     9https://www.anthropic.com/, https://aws. amazon.com/bedrock/     zero-shot and the few-shot prompts designed for evaluating the dataset, we use Claude 3 Sonnet, Haiku, Llama-3.1 70B, and LLama-3-1-8B with a greedy decoding strategy, i.e., we set the top-p value to 1.0 and temperature to 0.0. We imple- ment the DIN-SQL model using Claude 3.5 Son- net, Claude 3 Sonnet, Llama3-1-70B, Llama3-1- 8B, and Mixtral-Large-2 by tailoring the original GPT-4 based prompts and using the same set of hyperparameters as that used by GPT-4. Future work can focus on evaluating our dataset with the DIN-SQL model implemented using GPT-4.     \",\n",
      "  \"This setting allows us to assess how well the model performs if cell value retrieval is perfect.     4.2 SQL Prediction We use the DIN-SQL prompt-based framework, a SoTA method on the Spider dataset for predicting the final clarification SQL (Pourreza and Rafiei,     5https://github.com/BerriAI/litellm     Figure 2: Figure showing the classification accuracy of different models using different number of shots.     2024). The framework takes as input user ques- tions and the corresponding database schema and contains four modules that decompose the task of SQL generation into several sub-tasks following a chain-of-thought (Wei et al., 2022) approach for SQL generation.     5 Results and Discussions     Figure 2 shows the question category classification accuracy of different LLMs using varying numbers of examples. Claude 3.5 Sonnet6 achieves the best accuracy of 77.4% (75.9% excluding answerable category) across all categories when Oracle cell val- ues are included in the schema and 3 examples per question type are provided. Without oracle cell val- ues, the accuracy drops to 74.3% (72.4% excluding answerable). Mixtral-large-v27 performs similarly to Claude 3 Sonnet when at least 1 example is pro- vided per category but outperforms other models in the zero-shot setting, except Claude 3.5 Son- net. For the average accuracy across all categories, having lexical cell values improves performance by 0.7%, although the results are mixed. Across the three subcategories where cell values play a significant role (ambiguous VALUES within col- umn, ambiguous WHERE column, and ambiguous filter criteria), having oracle cell values boosts clas- sification accuracy by 1.5%. These results show that improving cell value retrieval can be an impor- tant thing for detecting ambiguous/unanswerable questions in a practical text-to-SQL system, which     6https://www.anthropic.com/news/ claude-3-5-sonnet     7https://mistral.ai/news/mistral-large-2407/           https://www.anthropic.com/news/claude-3-5-sonnet         https://www.anthropic.com/news/claude-3-5-sonnet         https://mistral.ai/news/mistral-large-2407/Table 5: Execution accuracy of SQLs predicted with DIN-SQL using different LLMs on various categories of ambiguous, unanswerable, and answerable questions. The \\\"All\\\" column shows the overall average accuracy across all categories, while the \\\"Avg. Excluding Answerable\\\" column shows the average accuracy excluding the answerable questions from the Spider dataset.     Model Ambig. Filter Criteria     Ambig. SELECT Column     Ambig. Val- ues Within Column     Ambig. WHERE Column     Nonexist. Filter Value     Nonexist. SELECT Column     Nonexist. WHERE Column     Unsupported Join     Answerable Average Avg. Excluding Answerable     Claude 3.5 Sonnet 77.23% 67.25% 68.03% 77.14% 74.12% 64.73% 65.11% 76.53% 79.21% 72.15% 71.27% Claude 3 Sonnet 61.72% 58.48% 53.28% 59.05% 64.71% 55.19% 51.06% 77.46% 64.12% 60.56% 60.12% Llama-3.1 70B 68.65% 71.35% 63.11% 71.43% 65.88% 67.01% 69.36% 63.85% 76.31% 68.55% 67.58% Llama-3.1 8B 48.84% 55.56% 45.90% 59.05% 54.71% 48.76% 46.81% 56.34% 56.58% 52.50% 52.00% Mixtral-large-v2 75.91% 74.27% 69.67% 75.24% 71.76% 66.18% 65.53% 77.00% 78.72% 72.70% 71.95%     previous research has not focused much on. The open-source Llama-3.1 70B (Touvron et al.,     2023) model performs better than Mixtral-8x7b (Jiang et al., 2024) and Claude 3 Haiku but ex- hibits repeated text output when 2 or more ex- amples are provided, causing its performance to drop below 20%8. These results indicate that de- tecting fine-grained ambiguity/unanswerability in questions given a database remains challenging for most LLMs (accuracy < 60%), except for the pow- erful model Claude 3.5 Sonnet.     Table 5 shows our baseline method\\u2019s (DIN-SQL) performance on SQL prediction of various LLMs given the interaction between the user and the as- sistant. Overall, Mixtral-large-v2 and Claude 3.5 Sonnet achieve the highest average accuracy of 71.95% and 72.15% on the ambiguous/unanswer- able questions. Claude 3.5 sonnet achieves the high- est performance of 79.21% on the answerable ques- tions (original Spider dev set). The open-source model Llama-3.1 70B performs competitively on the answerable questions achieving 76.31% ac- curacy, only 2.9% lower than Claude 3.5 sonnet. However, it performs only at 67.58% accuracy on ambiguous/unanswerable questions, lagging 3.7% behind Claude 3.5 sonnet. The gap can be as large as 9% for some specific ambiguous question cat- egories, indicating room for improvement. Our framework can be used to generate training data to improve open-source models\\u2019 capabilities in both SQL prediction and detecting ambiguous/u- nanswerable questions.     6 Conclusion and Future work     In this work, we study current public text-to- SQL datasets and define four ambiguous and four unanswerable categories. We propose a frame- work to construct a practical conversational text-to- SQL dataset, PRACTIQ, using both carefully con-     8With 3 examples per category, the results are similar, and the evaluation was stopped early for Llama-3.1 70B.     structed rules and Large Language Models (LLMs). We use the Spider dev dataset for constructing PRACTIQ and generate around 2,800 conversa- tional data samples. We evaluate our dataset on two core tasks, question category classification, and SQL prediction, and benchmark it using sev- eral SoTA LLMs.     \",\n",
      "  \"PRACTIQ: A Practical Conversational text-to-SQL dataset with Ambiguous and Unanswerable Queries     Mingwen Dong\\u2020\\u2217 Nischal Ashok Kumar\\u2021*     Yiqun Hu\\u2020, Anuj Chauhan\\u2020, Chung-Wei Hang\\u2020, Shuaichen Chang\\u2020, Lin Pan\\u2020, Wuwei Lan\\u2020, Henghui Zhu\\u2020, Jiarong Jiang\\u2020, Patrick Ng\\u2020, Zhiguo Wang\\u2020     \\u2021University of Massachusetts at Amherst, \\u2020Amazon Web Services nashokkumar@cs.umass.edu, {mingwd, jiarongj, zhiguow}@amazon.com     Abstract     Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. How- ever, real user questions can often be ambigu- ous with multiple interpretations or unanswer- able due to a lack of relevant data. In this work, we construct a practical conversational text-to- SQL dataset called PRACTIQ, consisting of am- biguous and unanswerable questions inspired by real-world user questions. We first identi- fied four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seek- ing clarification, the user\\u2019s clarification, and the assistant\\u2019s clarified SQL response with the natural language explanation of the execution results. For some ambiguous queries, we also directly generate helpful SQL responses, that consider multiple aspects of ambiguity, instead of requesting user clarification. To benchmark the performance on ambiguous, unanswerable, and answerable questions, we implemented large language model (LLM)-based baselines using various LLMs. Our approach involves two steps: question category classification and clarification SQL prediction. Our experiments reveal that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively. We will release our code for data generation and experiments on GitHub1.     1 Introduction     Text-to-SQL systems aim to convert natural lan- guage questions into SQL queries that can be used to query a database. The systems serve as an inter- face between users and databases to allow the users     *Co-first authors with equal contribution. Work done while Nischal Ashok Kumar was an intern at AWS.     1https://github.com/amazon-science/ conversational-ambiguous-unanswerable-text2sql     access to information from the databases through their natural language questions. The advent of Large Language Models (LLMs) (Bubeck et al., 2023) has significantly enhanced the capabilities of text-to-SQL systems, such as DIN-SQL (Pourreza and Rafiei, 2024), achieving state-of-the-art (SoTA) performance on standard benchmarks2, including Spider (Yu et al., 2018) and BIRD (Li et al., 2024).     Although the SoTA text-to-SQL systems per- form well on clean benchmarks that contain only answerable user queries, they are still not well- equipped to deal with practical real-world data which have ambiguous or unanswerable questions (Wang et al., 2023a). The poor performance of SoTA text-to-SQL systems is primarily due to the unavailability of practical text-to-SQL data that can be used for training (Wang et al., 2023a). Al- though previous research finds that a large ratio of user questions are unanswerable, these are often ex- cluded in the previous datasets as addressing them requires more than SQL annotations (Lee et al., 2021). To bridge this gap, we introduce PRACTIQ which is a practical conversational text-to-SQL dataset with ambiguous and unanswerable queries. As illustrated in Table 2, a question is ambiguous if it has multiple valid interpretations given the database schema and the question is unanswerable if the corresponding database does not contain the data that the question is asking for. In the real world, given a user question, a text-to-SQL assis- tant has to first determine whether the question is answerable, ambiguous, or unanswerable to decide whether to ask for clarification questions or respond with the correct SQL.     We begin by examining existing text-to-SQL datasets (Yu et al., 2018; Li et al., 2024; Yu et al., 2019a) and identify four ambiguous and four unan- swerable categories inspired by real-world practical user questions. Subsequently, we generate ambigu-     2As of August 2023           https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sql         https://github.com/amazon-science/conversational-ambiguous-unanswerable-text2sqlTable Visitor     ID, Name, Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: SELECT Name, Age from Visitors WHERE ID LIKE 'B%';     Age     Age_at_Entry Current_Age     LLM     Table Visitor     ID, Name, Age_at_Entry, Current_Age     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: <Fill Here>     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: I find two matching columns for Age - Age_at_Entry and Current_age     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     Text-to-SQL data     U: Find the name and age of visitors whose ID starts with 'B'     A: For the visitor's age, would you like their age at entry or their current age?     U: I am interested in the age they entered.     A: SELECT Name, Age_at_Entry from Visitors WHERE ID LIKE 'B%';     A: [('Jack', 25), ('Jane', 24)]     A: The name and age at entry of visitors whose ID starts with 'B' are Jack with age 25 and Jane with age 24.     \"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Load the knowledge base ID from variables.json\n",
    "try:\n",
    "    with open(\"variables.json\", \"r\") as f:\n",
    "        variables = json.load(f)\n",
    "    kb_id = variables.get(\"kbHierarchicalChunk\")  # Get the hierarchical kb ID\n",
    "    \n",
    "    if not kb_id:\n",
    "        print(\"Knowledge base ID not found in variables.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading knowledge base ID: {e}\")\n",
    "    kb_id = None\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query for document retrieval\n",
    "query = \"What were net incomes of Amazon in 2022, 2023 and 2024?\" \n",
    "\n",
    "# Retrieve relevant documents from the knowledge base\n",
    "relevant_documents_os = bedrock_agent_runtime.retrieve(\n",
    "    retrievalQuery={\n",
    "        'text': query  # Search query\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,  # Use the retrieved knowledge base ID instead of kb['knowledgeBaseId']\n",
    "    retrievalConfiguration={\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3  # Limit to top 3 results\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the retrieved documents\n",
    "print(json.dumps([i[\"content\"][\"text\"] for i in relevant_documents_os[\"retrievalResults\"]], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55a037",
   "metadata": {},
   "source": [
    "> **Note**: After creating the knowledge base, you can explore its details and settings in the Amazon Bedrock console. This gives you a more visual interface to understand how the knowledge base is structured.\n",
    "> \n",
    "> **[➡️ View your Knowledge Bases in the AWS Console](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/knowledge-bases)**\n",
    ">\n",
    "> In the console, you can:\n",
    "> - See all your knowledge bases in one place\n",
    "> - View ingestion status and statistics\n",
    "> - Test queries through the built-in chat interface\n",
    "> - Modify settings and configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

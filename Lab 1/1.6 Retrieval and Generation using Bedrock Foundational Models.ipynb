{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fe8a71-6170-4caf-a3b9-e578c1c5f201",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context.\n",
    "\n",
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66330105-e1f4-46f3-9b36-9f7560407522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '307297743176',\n",
       " 'regionName': 'us-west-2',\n",
       " 'collectionArn': 'arn:aws:aoss:us-west-2:307297743176:collection/h7cmj732p9d3v91spkhd',\n",
       " 'collectionId': 'h7cmj732p9d3v91spkhd',\n",
       " 'vectorIndexName': 'ws-index-',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::307297743176:role/advanced-rag-workshop-bedrock_execution_role-us-west-2',\n",
       " 's3Bucket': '307297743176-us-west-2-advanced-rag-workshop',\n",
       " 'kbFixedChunk': '4P6PBDDEGL',\n",
       " 'kbSemanticChunk': 'IC3ZCBORXT',\n",
       " 'kbCustomChunk': 'Q2T9CZ5VFA',\n",
       " 'kbHierarchicalChunk': '1YIFVW0Z5E'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03249f43",
   "metadata": {},
   "source": [
    "## RAG with a simple question\n",
    "\n",
    "##### We will ask the question \"In text-to-sql, what are the stages in data generation process?\" <br/>\n",
    "##### We should expect a response from a PDF shown below that includes the three stages shown in picture below.\n",
    "![Image](./image01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12750c99",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4199b3bd-3f66-4a29-9929-83cb9efa5723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knowledge Base ID - Choose from different chunking strategies (Fixed, Hierarchical, or Semantic)\n",
    "kb_id = variables[\"kbFixedChunk\"] \n",
    "\n",
    "# Bedrock Model ARN - Using Amazon Nova Lite for inference\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Number of relevant documents to retrieve for RAG\n",
    "number_of_results = 5\n",
    "\n",
    "# Configuration for text generation - Controls output length, randomness, and diversity\n",
    "generation_configuration = {\n",
    "    'inferenceConfig': {\n",
    "        'textInferenceConfig': {\n",
    "            'maxTokens': 4096,  # Maximum number of tokens in the generated response\n",
    "            'stopSequences': [],  # List of sequences that indicate stopping points\n",
    "            'temperature': 0.2,  # Controls randomness (lower values = more deterministic output)\n",
    "            'topP': 0.5  # Controls diversity of output by considering top P probability mass\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29db82",
   "metadata": {},
   "source": [
    "### Retrieve and Generate with a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de00f7db-57bc-4a91-b9dc-6ba3fef14917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Answer ---------------------\n",
      "Answer: The data generation process in text-to-SQL consists of three main stages:\n",
      "\n",
      "1. **SQL parsing & Database modification**: The first step involves extracting the columns and cell values by parsing the SQL queries using a custom parser on top of SQLGLOT. Then, the database schemas are modified using an LLM to create ambiguous or unanswerable questions. For example, for Ambiguous SELECT Column questions, the LLM generates two alternative columns to replace the original column mentioned in the question, making it ambiguous.\n",
      "\n",
      "2. **SQL modification and clarification response generation**: Based on the user question, the modified database, and the original SQL, the text-to-SQL assistant's initial response to the ambiguous/unanswerable question, the following user clarification response, and the assistant's SQL response to the clarified question are generated. The assistant's response to the initial user question is generated using either a template-based method or a prompting method. For example, for Ambiguous SELECT Column questions, the template is \"I find two conflicting information in the data. Which one would you like to know about? Ambiguous_SELECT_Column_1 or Ambiguous_SELECT_Column_2\".\n",
      "\n",
      "3. **Refining the conversation & Quality Control**: Leveraging an LLM, as a post-processing step, the naturalness and coherence of the conversation are improved, and a natural language explanation of the final SQL execution results is added. The LLM is used to evaluate the quality of the data generated from the previous step or rank different candidates if multiple candidates have been generated. For example, for an ambiguous SELECT column question, the LLM evaluates whether the generated columns are good candidates and make the question ambiguous.\n",
      "\n",
      "----------------- Citations ------------------\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"305e0d13-70ea-4de6-90e7-9af69073454b\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Mon, 07 Apr 2025 15:58:31 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"21572\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"305e0d13-70ea-4de6-90e7-9af69073454b\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"citations\": [\n",
      "    {\n",
      "      \"generatedResponsePart\": {\n",
      "        \"textResponsePart\": {\n",
      "          \"span\": {\n",
      "            \"end\": 529,\n",
      "            \"start\": 0\n",
      "          },\n",
      "          \"text\": \"Answer: The data generation process in text-to-SQL consists of three main stages:\\n\\n1. **SQL parsing & Database modification**: The first step involves extracting the columns and cell values by parsing the SQL queries using a custom parser on top of SQLGLOT. Then, the database schemas are modified using an LLM to create ambiguous or unanswerable questions. For example, for Ambiguous SELECT Column questions, the LLM generates two alternative columns to replace the original column mentioned in the question, making it ambiguous\"\n",
      "        }\n",
      "      },\n",
      "      \"retrievedReferences\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"text\": \"Lastly, we handle cases with ambiguous inputs that can be addressed without explicitly needing a user clar- ification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SE- LECT Column and Ambiguous WHERE Column questions. Table 1 compares the ambiguous and unanswerable categories defined in our work with     existing datasets, highlighting the range of broader categories covered in PRACTIQ. By addressing the limitations of existing datasets and providing a comprehensive and conversational dataset, our work aims to support the development of practical text-to-SQL applications that can handle ambigu- ous and unanswerable queries more effectively.     3 Question Categorization & Dataset Construction     We analyzed public text-to-SQL datasets like Spi- der (Yu et al., 2018), BIRD (Li et al., 2024), CoSQL (Yu et al., 2019a) and proposed four ambiguous and four unanswerable categories, as shown in Table 2. The ambiguous categories include Am- biguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria. Ambiguous questions have multiple possible interpretations and subse- quently multiple correct SQL responses given the database schema. The unanswerable categories in- clude Nonexistent SELECT Column, Nonexistent WHERE Column, Nonexistent Filter Value, and Unsupported Join. Unanswerable questions are those for which a valid SQL cannot be produced given the database schema.     The data generation process consists of three main stages, as shown in Figure 1. We describe the main procedure and illustrate it with a detailed explanation for one category. For convenience, we use \\\"assistant\\\" to indicate the text-to-SQL system in the remaining text. Please see Appendix E for a detailed explanation of the data generation process for each category.     3.1 Stage 1: SQL parsing & Database modification     We first extract the columns and cell values by pars- ing the SQL queries using a custom parser on top of SQLGLOT4. Then, we select a column or cell value of interest and modify the database schemas using an LLM so that the question becomes ambiguous or unanswerable. Since users are often unaware of database details, modifying the databases instead of the user questions, when plausible, is a natural way to create ambiguous and unanswerable questions. For example, for Ambiguous SELECT Column questions, we asked the LLM to generate two al- ternative columns to replace the original column mentioned in the question, such that either column     4https://github.com/tobymao/sqlglotis a valid interpretation of the question (see Prompt 4 for details). For Nonexistent Filter Value ques- tions, we remove the mentioned cell values from the database, making the question unanswerable. For example, given the user question \\\"What is the maximum capacity of all stadiums?\\\" and the origi- nal database schema with the column \\\"Capacity\\\", we prompt the LLM to generate two semantically similar but non-equivalent columns, \\\"Standing Ca- pacity\\\" and \\\"Seating Capacity\\\". We then remove the original \\\"Capacity\\\" column and add the newly generated columns to the database.     3.2 Stage 2: SQL modification and clarification response generation     Based on the user question, the modified database, and the original SQL, we generate the text-to-SQL assistant\\u2019s initial response to the ambiguous/unan- swerable question, the following user clarification response, and the assistant\\u2019s SQL response to the clarified question. First, we generate the assistant\\u2019s response to the initial user question using either a template-based method or a prompting method. For example, for Ambiguous SELECT Column questions, the template is \\\"I find two conflicting information in the data. Which one would you like to know about? Ambiguous_SELECT_Column_1 or Ambiguous_SELECT_Column_2\\\".     Next, we follow a reverse-generation process (Hu et al., 2023) to first generate the assistant\\u2019s final SQL response and then generate the user\\u2019s clarification question. The assistant\\u2019s final SQL response is generated by modifying the original SQL programmatically. Then, we prompt the LLM to fill in the user\\u2019s clarification response based on the conversation context (initial user question, as- sistant\\u2019s clarification question, and final SQL re- sponses). For example, for the Ambiguous SE- LECT Column question, we generate the assis- tant\\u2019s clarified SQL by replacing the column in the SELECT clause of the original SQL with one of the ambiguous SELECT columns generated in the above stage. Then, given the user\\u2019s ini- tial question, the assistant\\u2019s clarification question, \\\"empty_user_clarification_response\\\", and the assis- tant\\u2019s final SQL response, we prompt the LLM to fill in the \\\"empty_user_clarification_response\\\" so that the user clarification response matches the as- sistant\\u2019s SQL response and rest of the conversation (see Prompt 5 for details).\",\n",
      "            \"type\": \"TEXT\"\n",
      "          },\n",
      "          \"location\": {\n",
      "            \"s3Location\": {\n",
      "              \"uri\": \"s3://307297743176-us-west-2-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\"\n",
      "            },\n",
      "            \"type\": \"S3\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"x-amz-bedrock-kb-source-uri\": \"s3://307297743176-us-west-2-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\",\n",
      "            \"x-amz-bedrock-kb-document-page-number\": 4.0,\n",
      "            \"year\": 2025.0,\n",
      "            \"docType\": \"science\",\n",
      "            \"x-amz-bedrock-kb-data-source-id\": \"KBFRA7RZBF\",\n",
      "            \"company\": \"Amazon\",\n",
      "            \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3A2pPbEJYBqeJUJcrbqsl7\",\n",
      "            \"authors\": [\n",
      "              \"Marvin Dong\",\n",
      "              \"Nischal Ashok Kumar\",\n",
      "              \"Yiqun Hu\",\n",
      "              \"Anuj Chauhan\",\n",
      "              \"Chung-Wei Hang\",\n",
      "              \"Shuaichen Chang\",\n",
      "              \"Lin Pan\",\n",
      "              \"Wuwei Lan\",\n",
      "              \"Henry Zhu\",\n",
      "              \"Jiarong Jiang\",\n",
      "              \"Patrick Ng\",\n",
      "              \"Zhiguo Wang\"\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"generatedResponsePart\": {\n",
      "        \"textResponsePart\": {\n",
      "          \"span\": {\n",
      "            \"end\": 1210,\n",
      "            \"start\": 531\n",
      "          },\n",
      "          \"text\": \"2. **SQL modification and clarification response generation**: Based on the user question, the modified database, and the original SQL, the text-to-SQL assistant's initial response to the ambiguous/unanswerable question, the following user clarification response, and the assistant's SQL response to the clarified question are generated. The assistant's response to the initial user question is generated using either a template-based method or a prompting method. For example, for Ambiguous SELECT Column questions, the template is \\\"I find two conflicting information in the data. Which one would you like to know about? Ambiguous_SELECT_Column_1 or Ambiguous_SELECT_Column_2\\\"\"\n",
      "        }\n",
      "      },\n",
      "      \"retrievedReferences\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"text\": \"Lastly, we handle cases with ambiguous inputs that can be addressed without explicitly needing a user clar- ification by directly generating helpful SQL and natural language responses covering all ambiguous columns in the database for the Ambiguous SE- LECT Column and Ambiguous WHERE Column questions. Table 1 compares the ambiguous and unanswerable categories defined in our work with     existing datasets, highlighting the range of broader categories covered in PRACTIQ. By addressing the limitations of existing datasets and providing a comprehensive and conversational dataset, our work aims to support the development of practical text-to-SQL applications that can handle ambigu- ous and unanswerable queries more effectively.     3 Question Categorization & Dataset Construction     We analyzed public text-to-SQL datasets like Spi- der (Yu et al., 2018), BIRD (Li et al., 2024), CoSQL (Yu et al., 2019a) and proposed four ambiguous and four unanswerable categories, as shown in Table 2. The ambiguous categories include Am- biguous SELECT Column, Ambiguous WHERE Columns, Ambiguous Values Within Columns, and Ambiguous Filter Criteria. Ambiguous questions have multiple possible interpretations and subse- quently multiple correct SQL responses given the database schema. The unanswerable categories in- clude Nonexistent SELECT Column, Nonexistent WHERE Column, Nonexistent Filter Value, and Unsupported Join. Unanswerable questions are those for which a valid SQL cannot be produced given the database schema.     The data generation process consists of three main stages, as shown in Figure 1. We describe the main procedure and illustrate it with a detailed explanation for one category. For convenience, we use \\\"assistant\\\" to indicate the text-to-SQL system in the remaining text. Please see Appendix E for a detailed explanation of the data generation process for each category.     3.1 Stage 1: SQL parsing & Database modification     We first extract the columns and cell values by pars- ing the SQL queries using a custom parser on top of SQLGLOT4. Then, we select a column or cell value of interest and modify the database schemas using an LLM so that the question becomes ambiguous or unanswerable. Since users are often unaware of database details, modifying the databases instead of the user questions, when plausible, is a natural way to create ambiguous and unanswerable questions. For example, for Ambiguous SELECT Column questions, we asked the LLM to generate two al- ternative columns to replace the original column mentioned in the question, such that either column     4https://github.com/tobymao/sqlglotis a valid interpretation of the question (see Prompt 4 for details). For Nonexistent Filter Value ques- tions, we remove the mentioned cell values from the database, making the question unanswerable. For example, given the user question \\\"What is the maximum capacity of all stadiums?\\\" and the origi- nal database schema with the column \\\"Capacity\\\", we prompt the LLM to generate two semantically similar but non-equivalent columns, \\\"Standing Ca- pacity\\\" and \\\"Seating Capacity\\\". We then remove the original \\\"Capacity\\\" column and add the newly generated columns to the database.     3.2 Stage 2: SQL modification and clarification response generation     Based on the user question, the modified database, and the original SQL, we generate the text-to-SQL assistant\\u2019s initial response to the ambiguous/unan- swerable question, the following user clarification response, and the assistant\\u2019s SQL response to the clarified question. First, we generate the assistant\\u2019s response to the initial user question using either a template-based method or a prompting method. For example, for Ambiguous SELECT Column questions, the template is \\\"I find two conflicting information in the data. Which one would you like to know about? Ambiguous_SELECT_Column_1 or Ambiguous_SELECT_Column_2\\\".     Next, we follow a reverse-generation process (Hu et al., 2023) to first generate the assistant\\u2019s final SQL response and then generate the user\\u2019s clarification question. The assistant\\u2019s final SQL response is generated by modifying the original SQL programmatically. Then, we prompt the LLM to fill in the user\\u2019s clarification response based on the conversation context (initial user question, as- sistant\\u2019s clarification question, and final SQL re- sponses). For example, for the Ambiguous SE- LECT Column question, we generate the assis- tant\\u2019s clarified SQL by replacing the column in the SELECT clause of the original SQL with one of the ambiguous SELECT columns generated in the above stage. Then, given the user\\u2019s ini- tial question, the assistant\\u2019s clarification question, \\\"empty_user_clarification_response\\\", and the assis- tant\\u2019s final SQL response, we prompt the LLM to fill in the \\\"empty_user_clarification_response\\\" so that the user clarification response matches the as- sistant\\u2019s SQL response and rest of the conversation (see Prompt 5 for details).\",\n",
      "            \"type\": \"TEXT\"\n",
      "          },\n",
      "          \"location\": {\n",
      "            \"s3Location\": {\n",
      "              \"uri\": \"s3://307297743176-us-west-2-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\"\n",
      "            },\n",
      "            \"type\": \"S3\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"x-amz-bedrock-kb-source-uri\": \"s3://307297743176-us-west-2-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\",\n",
      "            \"x-amz-bedrock-kb-document-page-number\": 4.0,\n",
      "            \"year\": 2025.0,\n",
      "            \"docType\": \"science\",\n",
      "            \"x-amz-bedrock-kb-data-source-id\": \"KBFRA7RZBF\",\n",
      "            \"company\": \"Amazon\",\n",
      "            \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3A2pPbEJYBqeJUJcrbqsl7\",\n",
      "            \"authors\": [\n",
      "              \"Marvin Dong\",\n",
      "              \"Nischal Ashok Kumar\",\n",
      "              \"Yiqun Hu\",\n",
      "              \"Anuj Chauhan\",\n",
      "              \"Chung-Wei Hang\",\n",
      "              \"Shuaichen Chang\",\n",
      "              \"Lin Pan\",\n",
      "              \"Wuwei Lan\",\n",
      "              \"Henry Zhu\",\n",
      "              \"Jiarong Jiang\",\n",
      "              \"Patrick Ng\",\n",
      "              \"Zhiguo Wang\"\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"generatedResponsePart\": {\n",
      "        \"textResponsePart\": {\n",
      "          \"span\": {\n",
      "            \"end\": 1767,\n",
      "            \"start\": 1212\n",
      "          },\n",
      "          \"text\": \"3. **Refining the conversation & Quality Control**: Leveraging an LLM, as a post-processing step, the naturalness and coherence of the conversation are improved, and a natural language explanation of the final SQL execution results is added. The LLM is used to evaluate the quality of the data generated from the previous step or rank different candidates if multiple candidates have been generated. For example, for an ambiguous SELECT column question, the LLM evaluates whether the generated columns are good candidates and make the question ambiguous\"\n",
      "        }\n",
      "      },\n",
      "      \"retrievedReferences\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"text\": \"Next, we follow a reverse-generation process (Hu et al., 2023) to first generate the assistant\\u2019s final SQL response and then generate the user\\u2019s clarification question. The assistant\\u2019s final SQL response is generated by modifying the original SQL programmatically. Then, we prompt the LLM to fill in the user\\u2019s clarification response based on the conversation context (initial user question, as- sistant\\u2019s clarification question, and final SQL re- sponses). For example, for the Ambiguous SE- LECT Column question, we generate the assis- tant\\u2019s clarified SQL by replacing the column in the SELECT clause of the original SQL with one of the ambiguous SELECT columns generated in the above stage. Then, given the user\\u2019s ini- tial question, the assistant\\u2019s clarification question, \\\"empty_user_clarification_response\\\", and the assis- tant\\u2019s final SQL response, we prompt the LLM to fill in the \\\"empty_user_clarification_response\\\" so that the user clarification response matches the as- sistant\\u2019s SQL response and rest of the conversation (see Prompt 5 for details). This process ensures that the assistant\\u2019s clarified SQL is more accurate and executable, as we are not prompting the LLM     to generate it, which could lead to incorrect SQL. Finally, we execute the constructed clarification SQLs against the modified databases and discard examples that are not executable. After the reverse generation and filtering, each sample includes the user\\u2019s initial question, the assistant\\u2019s clarification question, the user\\u2019s clarification response, the as- sistant\\u2019s SQL response, and its corresponding exe- cution results.     3.2.1 Generating helpful SQL for ambiguous questions     Because it is not always helpful for the assistant to ask clarification questions for ambiguous/unan- swerable queries, we also generate helpful SQL responses to the Ambiguous SELECT Column and Ambiguous WHERE Column queries and reversely generate the corresponding assistant\\u2019s explanation of why the SQL response is helpful. For Am- biguous SELECT Column queries, we sometimes can simply return all valid interpretations of the columns in the SQL. For example, suppose the question \\\"What is the maximum capacity of all sta- diums?\\\" is ambiguous because capacity can map to either \\\"Standing Capacity\\\" or \\\"Seating Capacity\\\". In that case, we can return both capacity columns, reducing the number of turns for the user to get the information they need. We only generate such help- ful SQL responses for the Ambiguous SELECT Column and Ambiguous WHERE Column cate- gories, but this can be extended to other categories in the future.     3.3 Stage 3: Refining the conversation & Quality Control     Leveraging an LLM, as a post-processing step (Wang et al., 2023b), we use a 3-shot prompt to improve the naturalness and coherence of the con- versation and add a natural language explanation of the final SQL execution results (see Prompt 6 & 7 for details). We randomly select 3 examples of the original conversation (as obtained from Stage 2), rewrite it more naturally and coherently, and add a natural language explanation of the execution results.     In addition to the main steps for generating the data, we employ a separate evaluation step after each generation step to control the data quality besides optimizing the generation prompt. The fil- tering step uses both LLM and execution checks. The LLM is often used to evaluate the quality of the data generated from the previous step or rankdifferent candidates if multiple candidates have been generated. For example, for an ambiguous SELECT column question, suppose we have gener- ated \\\"Standing Capacity\\\" or \\\"Seating Capacity\\\" as alternative columns for the question \\\"What is the maximum capacity of all stadiums?\\\". We will have a separate prompt and a few-shot examples for the LLM to evaluate whether these two columns are good candidates and make the question ambigu- ous. For execution checks, whenever we make a database change or generate modified SQLs, we execute these SQLs against the modified database to ensure the SQLs are executable.     Lastly, after generating data for each category, we prompted a LLM to perform binary classifica- tion on whether the provided question and modified database pair belonged to the designed category or not. This classification was based on the definition of the category and several human-curated exam- ples (see Prompt 8 for details). We only retained the examples that passed this binary classification, ensuring that the generated data accurately repre- sented the intended ambiguous or unanswerable category.     3.4 Dataset Statistics Table 3 shows the statistics of the dataset generated using the Spider dev set with Claude 3 sonnet. Note that the employed methodology can be seamlessly adapted to other text-to-SQL datasets like BIRD, WikiSQL, or any other synthetically generated an- swerable text-to-SQL corpora combined with any LLM (e.g., Llama3.1 or mixtral).\",\n",
      "            \"type\": \"TEXT\"\n",
      "          },\n",
      "          \"location\": {\n",
      "            \"s3Location\": {\n",
      "              \"uri\": \"s3://307297743176-us-west-2-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\"\n",
      "            },\n",
      "            \"type\": \"S3\"\n",
      "          },\n",
      "          \"metadata\": {\n",
      "            \"x-amz-bedrock-kb-source-uri\": \"s3://307297743176-us-west-2-advanced-rag-workshop/data/pdf_documents/practiq-a-practical-conversational-text-to-sql-dataset-with-ambiguous-and-unanswerable-queries.pdf\",\n",
      "            \"x-amz-bedrock-kb-document-page-number\": 5.0,\n",
      "            \"year\": 2025.0,\n",
      "            \"docType\": \"science\",\n",
      "            \"x-amz-bedrock-kb-data-source-id\": \"KBFRA7RZBF\",\n",
      "            \"company\": \"Amazon\",\n",
      "            \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3A25PbEJYBqeJUJcrbqsl7\",\n",
      "            \"authors\": [\n",
      "              \"Marvin Dong\",\n",
      "              \"Nischal Ashok Kumar\",\n",
      "              \"Yiqun Hu\",\n",
      "              \"Anuj Chauhan\",\n",
      "              \"Chung-Wei Hang\",\n",
      "              \"Shuaichen Chang\",\n",
      "              \"Lin Pan\",\n",
      "              \"Wuwei Lan\",\n",
      "              \"Henry Zhu\",\n",
      "              \"Jiarong Jiang\",\n",
      "              \"Patrick Ng\",\n",
      "              \"Zhiguo Wang\"\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"output\": {\n",
      "    \"text\": \"Answer: The data generation process in text-to-SQL consists of three main stages:\\n\\n1. **SQL parsing & Database modification**: The first step involves extracting the columns and cell values by parsing the SQL queries using a custom parser on top of SQLGLOT. Then, the database schemas are modified using an LLM to create ambiguous or unanswerable questions. For example, for Ambiguous SELECT Column questions, the LLM generates two alternative columns to replace the original column mentioned in the question, making it ambiguous.\\n\\n2. **SQL modification and clarification response generation**: Based on the user question, the modified database, and the original SQL, the text-to-SQL assistant's initial response to the ambiguous/unanswerable question, the following user clarification response, and the assistant's SQL response to the clarified question are generated. The assistant's response to the initial user question is generated using either a template-based method or a prompting method. For example, for Ambiguous SELECT Column questions, the template is \\\"I find two conflicting information in the data. Which one would you like to know about? Ambiguous_SELECT_Column_1 or Ambiguous_SELECT_Column_2\\\".\\n\\n3. **Refining the conversation & Quality Control**: Leveraging an LLM, as a post-processing step, the naturalness and coherence of the conversation are improved, and a natural language explanation of the final SQL execution results is added. The LLM is used to evaluate the quality of the data generated from the previous step or rank different candidates if multiple candidates have been generated. For example, for an ambiguous SELECT column question, the LLM evaluates whether the generated columns are good candidates and make the question ambiguous.\"\n",
      "  },\n",
      "  \"sessionId\": \"3a73f0f2-034c-4ede-bc06-00c47db319d8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query to search relevant knowledge base documents and generate an answer\n",
    "query = \"In text-to-sql, what are the stages in data generation process?\"\n",
    "\n",
    "# Perform retrieval-augmented generation (RAG) using the knowledge base\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": generation_configuration,  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)\n",
    "\n",
    "# Display the full response including citations for retrieved documents\n",
    "print('----------------- Citations ------------------')\n",
    "print(json.dumps(response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7e3a8-5b2f-42de-83dd-e85344ffcdcd",
   "metadata": {},
   "source": [
    "### Comparison between chunking strategies: Fixed vs Semantic\n",
    "\n",
    "##### Now, Let's ask a more nuanced question that needs to extract information from a table in the PDF. Also, let's ask it to do some analysis. <br/>\n",
    "##### We will also compare the response quality when you use fixed size chunking vs Semantic chunking.\n",
    "![image02](image02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e88b9-613a-42cc-ac54-f827902f572f",
   "metadata": {},
   "source": [
    "#### A nuanced query with a Fixed-sized chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb27652-e52e-4dcf-a0e1-6c29f473b52c",
   "metadata": {},
   "source": [
    "##### We will ask question that should answer how net income changed rom 2022 to 2023 to 20234.\n",
    "![image03](image03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02bd4f32-234b-4719-abd5-f50d91206aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knowledge Base ID - Fixed Chunk.\n",
    "kb_id = variables[\"kbFixedChunk\"] \n",
    "\n",
    "# Bedrock Model ARN - Using Amazon Nova Lite for inference\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Number of relevant documents to retrieve for RAG\n",
    "number_of_results = 5\n",
    "\n",
    "# Configuration for text generation - Controls output length, randomness, and diversity\n",
    "generation_configuration = {\n",
    "    'inferenceConfig': {\n",
    "        'textInferenceConfig': {\n",
    "            'maxTokens': 4096,  # Maximum number of tokens in the generated response\n",
    "            'stopSequences': [],  # List of sequences that indicate stopping points\n",
    "            'temperature': 0.2,  # Controls randomness (lower values = more deterministic output)\n",
    "            'topP': 0.5  # Controls diversity of output by considering top P probability mass\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acba9900-a4ea-4f40-95f0-e8ad7b8b621f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Answer ---------------------\n",
      "Answer: The net income for the years 2022, 2023, and 2024 was $(2,722) million, $30,425 million, and $30,425 million, respectively. The net income increased by $33,147 million from 2022 to 2023 and remained the same from 2023 to 2024.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query to search relevant knowledge base documents and generate an answer\n",
    "query = \"In CONSOLIDATED STATEMENTS OF CASH FLOWS, How much did net income change in years 2022, 2023, 2024?\"\n",
    "\n",
    "# Perform retrieval-augmented generation (RAG) using the knowledge base\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": generation_configuration,  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51bc3d-39ee-4791-a763-f8285f130e29",
   "metadata": {},
   "source": [
    "#### The response above might not be accurate with what it should be.The accurate response should be:\n",
    "\n",
    "> Year 2022 to Year 2023: \\\\$33,147 increase<br/>\n",
    "Year 2023 to Year 2024: \\\\$28,823 increase \n",
    "\n",
    "#### Now Let's execute the same question while using the KB with Semantic Chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc31f91-f6b0-4b7f-9b20-1732db0c2b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knowledge Base ID - Semantic Chunk.\n",
    "kb_id = variables[\"kbSemanticChunk\"] \n",
    "\n",
    "\n",
    "# Bedrock Model ARN - Using Amazon Nova Lite for inference\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Number of relevant documents to retrieve for RAG\n",
    "number_of_results = 5\n",
    "\n",
    "# Configuration for text generation - Controls output length, randomness, and diversity\n",
    "generation_configuration = {\n",
    "    'inferenceConfig': {\n",
    "        'textInferenceConfig': {\n",
    "            'maxTokens': 4096,  # Maximum number of tokens in the generated response\n",
    "            'stopSequences': [],  # List of sequences that indicate stopping points\n",
    "            'temperature': 0.2,  # Controls randomness (lower values = more deterministic output)\n",
    "            'topP': 0.5  # Controls diversity of output by considering top P probability mass\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab14a1f-ea0c-4361-80b7-5d315beaad88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Answer ---------------------\n",
      "Answer: Here is the change in net income for the years 2022, 2023, and 2024:\n",
      "\n",
      "- 2022: Net income was $33,364 million in 2021 and $-2,722 million in 2022. So the change in net income was $-2,722 - $33,364 = -$36,086 million.\n",
      "- 2023: Net income was $-2,722 million in 2022 and $30,425 million in 2023. So the change in net income was $30,425 - $-2,722 = $33,147 million.\n",
      "- 2024: Net income was $30,425 million in 2023 and $59,248 million in 2024. So the change in net income was $59,248 - $30,425 = $28,823 million.\n",
      "\n",
      "So in summary, the change in net income was -$36,086 million in 2022, $33,147 million in 2023, and $28,823 million in 2024.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query to search relevant knowledge base documents and generate an answer\n",
    "query = \"In CONSOLIDATED STATEMENTS OF CASH FLOWS, How much did net income change in years 2022, 2023, 2024? Show me how you did the math.\"\n",
    "\n",
    "# Perform retrieval-augmented generation (RAG) using the knowledge base\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": generation_configuration,  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd5820-1e42-463d-8da3-0e59cebf3d27",
   "metadata": {},
   "source": [
    "Compare the above results with the accurate response that should be:\n",
    "> Year 2022 to Year 2023: \\\\$33,147 increase <br/>\n",
    "> Year 2023 to Year 2024: \\\\$28,823 increase\n",
    "\n",
    "As you can see here, Semantic Chunking was able to deliver accurate response as compared to Fixed Size chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c92234",
   "metadata": {},
   "source": [
    "## Improve RAG quality with Enhanced Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab045f5",
   "metadata": {},
   "source": [
    "### Importance of Prompt Engineering\n",
    "Prompt engineering refers to the practice of optimizing textual input to a large language model (LLM) to improve output and receive the responses you want. Prompting helps an LLM perform a wide variety of tasks, including classification, question answering, code generation, creative writing, and more. The quality of prompts that you provide to a LLM can impact the quality of the model's responses. <br/>\n",
    " \n",
    "\n",
    "### Useful techniques to improve prompts for Amazon Nova models\n",
    "Please refer [link](https://docs.aws.amazon.com/nova/latest/userguide/prompting.html) for the best practice of prompt engineering with Amazon Nova models. Fllowings are a few highlights:\n",
    "* Create precise prompts. Provide contextual information, speficy the output format and style, and provide clear prompt sections.\n",
    "* Use system propmts to define how the model will repond.\n",
    "* Give Amazon Nova time to think. For example, add ```\"Think step-by-step.\"``` at the end of your query.\n",
    "* Provide examples.\n",
    "\n",
    "### Tips for using prompts in RAG\n",
    "* Provide Prompt Template: As with other functionalities, enhancing the system prompt can be beneficial. You can define the RAG Systems description in the system prompt, outlining the desired persona and behavior for the model.\n",
    "* Use Model Instructions: Additionally, you can include a dedicated ```\"Model Instructions:\"``` section within the system prompt, where you can provide specific guidelines for the model to follow. For instance, you can list instructions such as: ```In this example session, the model has access to search results and a user's question, its job is to answer the user's question using only information from the search results.```\n",
    "* Avoid Hallucination by restricting the instructions: Bring more focus to instructions by clearly mentioning \"DO NOT USE INFORMATION THAT IS NOT IN SEARCH RESULTS!\" as a model instruction so the answers are grounded in the provided context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd73e50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A prompt template with Model Instructions:\n",
    "prompt_template = \"\"\"\n",
    "You are a professional financial analyst. \n",
    "Based on the retrieved content from Amazon's 10-K filings, provide clear, concise, and insightful answers to user questions. \n",
    "When summarizing financial results, respond in bullet points highlighting key metrics, trends, and takeaways. \n",
    "Ensure your answers are accurate, data-driven, and easy to understand.\n",
    "\n",
    "$Query$\n",
    "Resource: $search_results$\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa9d55-474d-403d-ac4f-ecf6a5914886",
   "metadata": {},
   "source": [
    "#### Without a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "676924ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Answer ---------------------\n",
      "Amazon's financial results for 2023 show a net sales of $574,785 million, with an operating income of $36,852 million.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Show me the amazon financial results for 2023\"\n",
    "\n",
    "# Perform RAG with/without the prompt template\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": {**generation_configuration\n",
    "                                        },  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d97251-3560-48c0-ad95-13f9ea4ca40a",
   "metadata": {},
   "source": [
    "#### Using a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "772c4a54-54bc-4aa4-aba2-695d29da0ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Answer ---------------------\n",
      "Based on the provided content from Amazon's 10-K filings, here are the financial results for 2023:\n",
      "\n",
      "### Amazon Financial Results for 2023\n",
      "\n",
      "#### Key Metrics:\n",
      "- **Net Sales:**\n",
      "  - As Reported: $574,785 million\n",
      "  - Exchange Rate Effect: $71 million\n",
      "  - At Prior Year Rates: $574,856 million\n",
      "\n",
      "- **Operating Expenses:**\n",
      "  - As Reported: $537,933 million\n",
      "  - Exchange Rate Effect: $531 million\n",
      "  - At Prior Year Rates: $538,464 million\n",
      "\n",
      "- **Operating Income:**\n",
      "  - As Reported: $36,852 million\n",
      "  - Exchange Rate Effect: $(460) million\n",
      "  - At Prior Year Rates: $36,392 million\n",
      "\n",
      "#### Trends and Takeaways:\n",
      "- **Net Sales:**\n",
      "  - Amazon's net sales increased from the prior year, showing a positive trend in revenue generation.\n",
      "  - The exchange rate had a minimal positive effect on reported net sales.\n",
      "\n",
      "- **Operating Expenses:**\n",
      "  - Operating expenses also increased, reflecting higher costs associated with running the business.\n",
      "  - The exchange rate had a slight positive effect on operating expenses.\n",
      "\n",
      "- **Operating Income:**\n",
      "  - Operating income saw a significant increase from the prior year, indicating improved operational efficiency.\n",
      "  - The exchange rate had a minor negative effect on operating income.\n",
      "\n",
      "These metrics highlight Amazon's growth in revenue and operating income, despite increased expenses, showcasing the company's ability to manage costs effectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform RAG with/without the prompt template\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": {**generation_configuration\n",
    "                                        , \"promptTemplate\":{\"textPromptTemplate\": prompt_template} # Comment in/out to test the effect of the Prompt Template\n",
    "                                    },  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec91e22-0030-4578-817c-d00bd888af1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

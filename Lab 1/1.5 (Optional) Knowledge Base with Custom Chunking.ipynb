{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd35c145-e59c-4c6f-8ce2-3662b5d36151",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with Custom chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c7ebc-5152-4564-bac5-14cb7261aa44",
   "metadata": {},
   "source": [
    "#### Custom Chunking Logic with Lambda Functions in Amazon Bedrock\n",
    "\n",
    "When creating a Knowledge Base (KB) for Amazon Bedrock, you can connect a Lambda function to specify your custom chunking logic. During the ingestion process, if a Lambda function is provided, the Knowledge Base will execute the Lambda function and store the input and output values in the specified intermediate S3 bucket.\n",
    "\n",
    "#### Use Cases for Lambda Functions in KBs\n",
    "\n",
    "- **Custom Chunking Logic:** Lambda functions can be used to implement custom logic for chunking documents during ingestion, enabling more control over how documents are divided into meaningful chunks.\n",
    "- **Chunk-level Metadata Processing:** Lambda functions can also process chunked data, for example, by adding custom metadata at the chunk level, enriching the data for more advanced retrieval or analysis.\n",
    "\n",
    "This allows for more flexibility and tailored handling of document data within the Knowledge Base, making it possible to apply unique chunking strategies and augment the data with specific metadata for improved search and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0b6f2e-3e64-4bb0-8f70-d7be425a4a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '307297743176',\n",
       " 'regionName': 'us-west-2',\n",
       " 'collectionArn': 'arn:aws:aoss:us-west-2:307297743176:collection/h7cmj732p9d3v91spkhd',\n",
       " 'collectionId': 'h7cmj732p9d3v91spkhd',\n",
       " 'vectorIndexName': 'ws-index-',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::307297743176:role/advanced-rag-workshop-bedrock_execution_role-us-west-2',\n",
       " 's3Bucket': '307297743176-us-west-2-advanced-rag-workshop',\n",
       " 'kbFixedChunk': '4P6PBDDEGL',\n",
       " 'kbSemanticChunk': 'IC3ZCBORXT'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b3661-448e-4717-90cd-e7b56ddf15da",
   "metadata": {},
   "source": [
    "### 0. Create a Lambda function with custom chunking logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da24d0d2-ec77-4ef5-b73f-a8877dc21d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new IAM role: advanced-rag-custom-chunk-us-west-2-role\n",
      "Creating new Lambda function: advanced-rag-custom-chunk\n",
      "Lambda function created successfully\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import botocore\n",
    "\n",
    "# Create IAM client to interact with AWS IAM service\n",
    "iam = boto3.client(\"iam\", region_name=variables[\"regionName\"])\n",
    "lambda_client = boto3.client(\"lambda\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the role name\n",
    "role_name = f\"advanced-rag-custom-chunk-{variables['regionName']}-role\"\n",
    "function_name = \"advanced-rag-custom-chunk\"\n",
    "\n",
    "# Try to get the IAM role if it exists\n",
    "try:\n",
    "    # Check if the role already exists\n",
    "    get_role_response = iam.get_role(RoleName=role_name)\n",
    "    lambda_iam_role = get_role_response  # Store the entire response\n",
    "    print(f\"IAM role '{role_name}' already exists. Using the existing role.\")\n",
    "except iam.exceptions.NoSuchEntityException:\n",
    "    # Define the IAM assume role policy for the Lambda function\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Convert the IAM assume role policy into JSON format\n",
    "    assume_role_policy_document_json = json.dumps(assume_role_policy_document)\n",
    "    \n",
    "    # Create the IAM role for the Lambda function with the assume role policy\n",
    "    lambda_iam_role = iam.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=assume_role_policy_document_json\n",
    "    )\n",
    "    print(f\"Created new IAM role: {role_name}\")\n",
    "\n",
    "# Always put the policy (it will update if it exists or create if it doesn't)\n",
    "iam.put_role_policy(\n",
    "    RoleName=role_name,  # Use role name directly instead of lambda_iam_role[\"Role\"][\"RoleName\"]\n",
    "    PolicyName=\"s3policy\",\n",
    "    PolicyDocument=json.dumps(\n",
    "        {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\", \n",
    "                        \"s3:PutObject\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        f\"arn:aws:s3:::{variables['s3Bucket']}-custom-chunk\",\n",
    "                        f\"arn:aws:s3:::{variables['s3Bucket']}-custom-chunk/*\"\n",
    "                    ],\n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": f\"{variables['accountNumber']}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prepare the Lambda function code by creating a ZIP file\n",
    "s = BytesIO()\n",
    "z = zipfile.ZipFile(s, 'w')\n",
    "z.write(\"lambda_function.py\")\n",
    "z.close()\n",
    "zip_content = s.getvalue()\n",
    "\n",
    "# Sleep for 10 seconds to ensure resources are available\n",
    "time.sleep(10)\n",
    "\n",
    "# Get the role ARN\n",
    "role_arn = lambda_iam_role[\"Role\"][\"Arn\"]\n",
    "\n",
    "# Check if the Lambda function already exists\n",
    "try:\n",
    "    lambda_client.get_function(FunctionName=function_name)\n",
    "    print(f\"Lambda function '{function_name}' already exists. Updating code...\")\n",
    "    \n",
    "    # Update existing function code\n",
    "    lambda_function = lambda_client.update_function_code(\n",
    "        FunctionName=function_name,\n",
    "        ZipFile=zip_content\n",
    "    )\n",
    "    print(\"Lambda function code updated successfully\")\n",
    "except lambda_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"Creating new Lambda function: {function_name}\")\n",
    "    \n",
    "    # Create the Lambda function\n",
    "    lambda_function = lambda_client.create_function(\n",
    "        FunctionName=function_name,\n",
    "        Runtime='python3.12',\n",
    "        Timeout=60,\n",
    "        Role=role_arn,\n",
    "        Code={'ZipFile': zip_content},\n",
    "        Handler='lambda_function.lambda_handler'\n",
    "    )\n",
    "    print(\"Lambda function created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325eb5bd-f3a6-4144-835d-5b6db60fd516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket '307297743176-us-west-2-advanced-rag-workshop-custom-chunk' created.\n"
     ]
    }
   ],
   "source": [
    "# Create an S3 client to interact with the AWS S3 service in the specified region\n",
    "s3 = boto3.client(\"s3\", region_name=variables[\"regionName\"])\n",
    "\n",
    "try:\n",
    "    # Check if the bucket already exists by sending a HEAD request to S3\n",
    "    s3.head_bucket(Bucket=variables[\"s3Bucket\"]+\"-custom-chunk\")\n",
    "    # If the bucket exists, print a message\n",
    "    print(f\"Bucket '{variables['s3Bucket']}' already exists.\")\n",
    "except:\n",
    "    # If the bucket does not exist, create a new one\n",
    "    s3.create_bucket(Bucket=variables[\"s3Bucket\"]+\"-custom-chunk\", CreateBucketConfiguration={\n",
    "        'LocationConstraint': variables[\"regionName\"]})  # Specify the region for the new bucket\n",
    "    # Print a message indicating the bucket has been created\n",
    "    print(f\"Bucket '{variables['s3Bucket']}-custom-chunk' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446f270-acf7-4278-a664-643169ee2646",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4a398d-a1b2-4552-9669-5316f303b6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function definition\n",
    "from retrying import retry  # Import retrying module to add retry logic\n",
    "import boto3  # Import boto3 for AWS SDK to interact with AWS services\n",
    "\n",
    "# Create a Bedrock agent client to interact with Amazon Bedrock service\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Retry logic added to the function, which will retry the function 3 times with a random wait time between 1-2 seconds\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=3)\n",
    "def create_knowledge_base_func(name, description, chunking_type):\n",
    "    # Define the embedding model ARN that will be used by Bedrock for embedding ingested documents\n",
    "    embedding_model_arn = f\"arn:aws:bedrock:{variables['regionName']}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "    \n",
    "    # Define OpenSearch Serverless configuration that includes the collection and vector index names\n",
    "    opensearch_serverless_configuration = {\n",
    "            \"collectionArn\": variables[\"collectionArn\"],  # ARN of the OpenSearch collection\n",
    "            \"vectorIndexName\": variables[\"vectorIndexName\"] + chunking_type,  # Name of the vector index\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",  # Field name for the vector\n",
    "                \"textField\": \"text\",      # Field name for the text\n",
    "                \"metadataField\": \"text-metadata\"  # Field name for the metadata\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Print the OpenSearch configuration for debugging purposes\n",
    "    print(opensearch_serverless_configuration)\n",
    "    \n",
    "    # Call the Bedrock API to create the knowledge base with the specified configurations\n",
    "    create_kb_response = bedrock_agent.create_knowledge_base(\n",
    "        name=name,  # Name of the knowledge base\n",
    "        description=description,  # Description of the knowledge base\n",
    "        roleArn=variables[\"bedrockExecutionRoleArn\"],  # ARN of the IAM role that Bedrock will use for execution\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",  # Type of the knowledge base (VECTOR in this case)\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embedding_model_arn  # ARN of the embedding model used for the knowledge base\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",  # Type of the storage (using OpenSearch Serverless)\n",
    "            \"opensearchServerlessConfiguration\": opensearch_serverless_configuration  # OpenSearch configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Return the created knowledge base details\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe36ef26-2058-454e-b4ba-1804e086fcd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collectionArn': 'arn:aws:aoss:us-west-2:307297743176:collection/h7cmj732p9d3v91spkhd', 'vectorIndexName': 'ws-index-custom', 'fieldMapping': {'vectorField': 'vector', 'textField': 'text', 'metadataField': 'text-metadata'}}\n",
      "OpenSearch Knowledge Response: {\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"c67f0032-7acb-4af8-9f49-37682d1d4728\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"date\": \"Mon, 07 Apr 2025 15:44:25 GMT\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"content-length\": \"958\",\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"x-amzn-requestid\": \"c67f0032-7acb-4af8-9f49-37682d1d4728\",\n",
      "            \"x-amz-apigw-id\": \"IqLh-ESrvHcEc_w=\",\n",
      "            \"x-amzn-trace-id\": \"Root=1-67f3f2d9-491f6f3b5b9eb1440d720c82\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"knowledgeBase\": {\n",
      "        \"createdAt\": \"2025-04-07 15:44:24.721731+00:00\",\n",
      "        \"description\": \"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
      "        \"knowledgeBaseArn\": \"arn:aws:bedrock:us-west-2:307297743176:knowledge-base/Q2T9CZ5VFA\",\n",
      "        \"knowledgeBaseConfiguration\": {\n",
      "            \"type\": \"VECTOR\",\n",
      "            \"vectorKnowledgeBaseConfiguration\": {\n",
      "                \"embeddingModelArn\": \"arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v2:0\"\n",
      "            }\n",
      "        },\n",
      "        \"knowledgeBaseId\": \"Q2T9CZ5VFA\",\n",
      "        \"name\": \"advanced-rag-workshop-custom-chunking\",\n",
      "        \"roleArn\": \"arn:aws:iam::307297743176:role/advanced-rag-workshop-bedrock_execution_role-us-west-2\",\n",
      "        \"status\": \"CREATING\",\n",
      "        \"storageConfiguration\": {\n",
      "            \"opensearchServerlessConfiguration\": {\n",
      "                \"collectionArn\": \"arn:aws:aoss:us-west-2:307297743176:collection/h7cmj732p9d3v91spkhd\",\n",
      "                \"fieldMapping\": {\n",
      "                    \"metadataField\": \"text-metadata\",\n",
      "                    \"textField\": \"text\",\n",
      "                    \"vectorField\": \"vector\"\n",
      "                },\n",
      "                \"vectorIndexName\": \"ws-index-custom\"\n",
      "            },\n",
      "            \"type\": \"OPENSEARCH_SERVERLESS\"\n",
      "        },\n",
      "        \"updatedAt\": \"2025-04-07 15:44:24.721731+00:00\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Create a knowledge base using the predefined function\n",
    "    kb = create_knowledge_base_func(\n",
    "        name=\"advanced-rag-workshop-custom-chunking\",\n",
    "        description=\"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
    "        chunking_type=\"custom\"\n",
    "    )\n",
    "\n",
    "    # Retrieve details of the newly created knowledge base\n",
    "    get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "\n",
    "    # Update the variables dictionary with the new knowledge base ID\n",
    "    variables[\"kbCustomChunk\"] = kb['knowledgeBaseId']\n",
    "\n",
    "    # Save updated variables to a JSON file, handling datetime serialization\n",
    "    with open(\"variables.json\", \"w\") as f:\n",
    "        json.dump(variables, f, indent=4, default=str)  # Convert datetime to string\n",
    "\n",
    "    # Print the retrieved knowledge base response in a readable format\n",
    "    print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "    \n",
    "except Exception as e:\n",
    "    # Check if error message indicates the knowledge base already exists\n",
    "    error_message = str(e).lower()\n",
    "    if any(phrase in error_message for phrase in [\"already exist\", \"duplicate\", \"already been created\"]):\n",
    "        print(\"Knowledge Base already exists. Retrieving its ID...\")\n",
    "        \n",
    "        # List all knowledge bases to find the one that already exists\n",
    "        list_kb_response = bedrock_agent.list_knowledge_bases()\n",
    "        \n",
    "        # Look for a knowledge base with the desired name\n",
    "        for existing_kb in list_kb_response.get('knowledgeBaseSummaries', []):\n",
    "            if existing_kb['name'] == \"advanced-rag-workshop-custom-chunking\":\n",
    "                kb_id = existing_kb['knowledgeBaseId']\n",
    "                print(f\"Found existing knowledge base with ID: {kb_id}\")\n",
    "                \n",
    "                # Get the details of the existing knowledge base\n",
    "                get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "                \n",
    "                # Read existing variables to preserve other fields\n",
    "                try:\n",
    "                    # Read existing variables\n",
    "                    with open(\"variables.json\", \"r\") as f:\n",
    "                        existing_variables = json.load(f)\n",
    "                except (FileNotFoundError, json.JSONDecodeError):\n",
    "                    # If file doesn't exist or is invalid JSON\n",
    "                    existing_variables = {}\n",
    "                \n",
    "                # Update only the custom chunking value\n",
    "                existing_variables[\"kbCustomChunk\"] = kb_id\n",
    "                                \n",
    "                # Write back all variables\n",
    "                with open(\"variables.json\", \"w\") as f:\n",
    "                    json.dump(existing_variables, f, indent=4, default=str)\n",
    "                \n",
    "                # Print the retrieved knowledge base response\n",
    "                print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "                break        \n",
    "        else:\n",
    "            print(\"Could not find a knowledge base with the specified name.\")\n",
    "    else:\n",
    "        # If it's a different error, re-raise it\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684676c-0c32-47aa-9d48-d2c38b5a6206",
   "metadata": {},
   "source": [
    "### 2. Create Datasources for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e081015d-a84e-4481-a6fd-381d86417450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using knowledge base ID: Q2T9CZ5VFA\n",
      "Checking for existing data sources in knowledge base Q2T9CZ5VFA...\n",
      "Creating new data source 'advanced-rag-example' with custom chunking...\n",
      "Custom chunking data source created successfully with ID: OLMFDC65VK\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create clients\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Load variables to get the correct knowledge base ID\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "# Use the correct knowledge base ID - kbCustomChunk\n",
    "kb_id = variables.get(\"kbCustomChunk\")\n",
    "if not kb_id:\n",
    "    print(\"Error: No knowledge base ID found for custom chunking!\")\n",
    "    raise ValueError(\"Knowledge base ID missing\")\n",
    "print(f\"Using knowledge base ID: {kb_id}\")\n",
    "\n",
    "# Define configurations\n",
    "custom_transformation_configuration = {\n",
    "    \"intermediateStorage\": {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{variables['s3Bucket']}-custom-chunk/\"\n",
    "        }\n",
    "    },\n",
    "    \"transformations\": [\n",
    "        {\n",
    "            \"transformationFunction\": {\n",
    "                \"transformationLambdaConfiguration\": {\n",
    "                    \"lambdaArn\": f\"arn:aws:lambda:{variables['regionName']}:{variables['accountNumber']}:function:advanced-rag-custom-chunk\"\n",
    "                }\n",
    "            },\n",
    "            \"stepToApply\": \"POST_CHUNKING\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "s3_configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{variables['s3Bucket']}\",\n",
    "    \"inclusionPrefixes\": [\"data\"]\n",
    "}\n",
    "data_source_name = \"advanced-rag-example\"\n",
    "\n",
    "# Check if data source already exists and delete if needed\n",
    "try:\n",
    "    print(f\"Checking for existing data sources in knowledge base {kb_id}...\")\n",
    "    list_ds_response = bedrock_agent.list_data_sources(knowledgeBaseId=kb_id)\n",
    "    \n",
    "    existing_ds = None\n",
    "    for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "        if ds['name'] == data_source_name:\n",
    "            existing_ds = ds\n",
    "            break\n",
    "    \n",
    "    if existing_ds:\n",
    "        print(f\"Found existing data source '{data_source_name}'. Deleting it...\")\n",
    "        bedrock_agent.delete_data_source(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=existing_ds[\"dataSourceId\"]\n",
    "        )\n",
    "        print(\"Waiting for data source deletion to complete...\")\n",
    "        time.sleep(20)\n",
    "        print(\"Data source deleted.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error while checking or deleting data source: {e}\")\n",
    "\n",
    "# Create the new data source\n",
    "try:\n",
    "    print(f\"Creating new data source '{data_source_name}' with custom chunking...\")\n",
    "    create_ds_response = bedrock_agent.create_data_source(\n",
    "        name=data_source_name,\n",
    "        description=\"A data source for Advanced RAG workshop\",\n",
    "        knowledgeBaseId=kb_id,\n",
    "        dataSourceConfiguration={\n",
    "            \"type\": \"S3\",\n",
    "            \"s3Configuration\": s3_configuration\n",
    "        },\n",
    "        vectorIngestionConfiguration={\n",
    "            \"chunkingConfiguration\": {\"chunkingStrategy\": \"NONE\"},\n",
    "            \"customTransformationConfiguration\": custom_transformation_configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ds_custom_chunk = create_ds_response[\"dataSource\"]\n",
    "    ds_id = ds_custom_chunk[\"dataSourceId\"]\n",
    "    print(f\"Custom chunking data source created successfully with ID: {ds_id}\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response.get('Error', {}).get('Code', '')\n",
    "    if error_code == 'ConflictException':\n",
    "        print(f\"Data source '{data_source_name}' already exists. Retrieving it...\")\n",
    "        list_ds_response = bedrock_agent.list_data_sources(knowledgeBaseId=kb_id)\n",
    "        for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "            if ds['name'] == data_source_name:\n",
    "                ds_custom_chunk = ds\n",
    "                ds_id = ds['dataSourceId']\n",
    "                print(f\"Retrieved existing data source with ID: {ds_id}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Error creating data source: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee4ce6-10ee-4c4b-ae9a-8d16be4ed48b",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769885e3",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in the OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d003980-60ba-4b7e-94c2-1bc99c0b63ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ingestion job for data source OLMFDC65VK...\n",
      "Ingestion job started with ID: UYRDRJCLD1\n",
      "Monitoring ingestion job status...\n",
      "Current status: STARTING - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Current status: IN_PROGRESS - waiting...\n",
      "Ingestion job completed successfully!\n",
      "Statistics: {'numberOfDocumentsDeleted': 0, 'numberOfDocumentsFailed': 0, 'numberOfDocumentsScanned': 7, 'numberOfMetadataDocumentsModified': 0, 'numberOfMetadataDocumentsScanned': 7, 'numberOfModifiedDocumentsIndexed': 0, 'numberOfNewDocumentsIndexed': 7}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Starting ingestion job for data source {ds_id}...\")\n",
    "    \n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        dataSourceId=ds_id\n",
    "    )\n",
    "    \n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(f\"Ingestion job started with ID: {job['ingestionJobId']}\")\n",
    "\n",
    "    print(\"Monitoring ingestion job status...\")\n",
    "    while job['status'] not in ['COMPLETE', 'FAILED', 'STOPPED']:\n",
    "        print(f\"Current status: {job['status']} - waiting...\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=ds_id,\n",
    "            ingestionJobId=job[\"ingestionJobId\"]\n",
    "        )\n",
    "        \n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "\n",
    "    if job['status'] == 'COMPLETE':\n",
    "        print(f\"Ingestion job completed successfully!\")\n",
    "        print(f\"Statistics: {job.get('statistics', 'No statistics available')}\")\n",
    "    else:\n",
    "        print(f\"Ingestion job ended with status: {job['status']}\")\n",
    "        print(f\"Failure reason: {job.get('failureReasons', 'No failure reason provided')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with ingestion job: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1edb27-a7f5-4ab4-8770-2de83134b430",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f7fad3-3114-4010-9870-5f52e5fb5e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Company\\u2019s calculations and data used to determine the amount of tax benefits to recognize. We evaluated the Company\\u2019s income tax disclosures in relation to these matters. /s/ Ernst & Young LLP We have served as the Company\\u2019s auditor since 1996. Seattle, Washington February 6, 2025 35Table of Contents AMAZON.COM, INC. CONSOLIDATED STATEMENTS OF CASH FLOWS (in millions) Year Ended December 31, 2022 2023 2024 CASH, CASH EQUIVALENTS, AND RESTRICTED CASH, BEGINNING OF PERIOD $ 36,477 $ 54,253 $ 73,890 OPERATING ACTIVITIES: Net income (loss) (2,722) 30,425 59,248 Adjustments to reconcile net income (loss) to net cash from operating activities: Depreciation\",\n",
      "  \"computation of earnings per share: Basic 10,005 10,117 10,189 Diluted 10,198 10,296 10,189 See accompanying notes to consolidated financial statements. 37Table of Contents AMAZON.COM, INC. CONSOLIDATED STATEMENTS OF COMPREHENSIVE INCOME (LOSS) (in millions) Year Ended December 31, 2020 2021 2022 Net income (loss) $ 21,331 $ 33,364 $ (2,722) Other comprehensive income (loss): Foreign currency translation adjustments, net of tax of $(36), $47, and $100 561 (819) (2,586) Net change in unrealized gains (losses) on available-for-sale debt securities: Unrealized gains (losses), net of tax of $(83), $72, and $159 273 (343) (823) Reclassification adjustment for losses (gains) included in \\u201cOther\",\n",
      "  \"AWS services and Amazon Prime memberships. Our total unearned revenue as of December 31, 2023 was $20.9 billion, of which $14.2 billion was recognized as revenue during the year ended December 31, 2024 and our total unearned revenue as of December 31, 2024 was $24.6 billion. Included in \\u201cOther long-term liabilities\\u201d on our consolidated balance sheets was $5.7 billion and $6.5 billion of unearned revenue as of December 31, 2023 and 2024. Additionally, we have performance obligations, primarily related to AWS, associated with commitments in customer contracts for future services that have not yet been recognized in our financial statements.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock agent runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "kb_id = variables.get(\"kbCustomChunk\")\n",
    "\n",
    "# Query for relevant documents\n",
    "query = \"What were net incomes of Amazon in 2022, 2023 and 2024?\" \n",
    "\n",
    "# Retrieve relevant documents based on the query from the knowledge base\n",
    "relevant_documents_os = bedrock_agent_runtime.retrieve(\n",
    "    retrievalQuery={\n",
    "        'text': query  # Specify the query text to search for relevant documents\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,  # Provide the knowledge base ID to search in\n",
    "    retrievalConfiguration={\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3  # Limit the results to top 3 documents closely matching the query\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Return the relevant documents fetched\n",
    "print(json.dumps([i[\"content\"][\"text\"] for i in relevant_documents_os[\"retrievalResults\"]], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e804b",
   "metadata": {},
   "source": [
    "> **Note**: After creating the knowledge base, you can explore its details and settings in the Amazon Bedrock console. This gives you a more visual interface to understand how the knowledge base is structured.\n",
    "> \n",
    "> **[➡️ View your Knowledge Bases in the AWS Console](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/knowledge-bases)**\n",
    ">\n",
    "> In the console, you can:\n",
    "> - See all your knowledge bases in one place\n",
    "> - View ingestion status and statistics\n",
    "> - Test queries through the built-in chat interface\n",
    "> - Modify settings and configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

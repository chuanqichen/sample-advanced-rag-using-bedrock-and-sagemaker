{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e83cb40-0f1d-4915-8e54-48c78541d349",
   "metadata": {},
   "source": [
    "# Bedrock Knowledge Base Retrieval and Generation with SageMaker Inference and Metadata Filtering  \n",
    "\n",
    "### Description:  \n",
    "This notebook showcases how to query and retrieve information from an Amazon Bedrock-powered knowledge base while leveraging SageMaker inference and metadata filtering. It covers key steps such as configuring queries, applying metadata filters, retrieving responses, and extracting citations used in the generated results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26234a2a",
   "metadata": {},
   "source": [
    "![Metadata Filtering](./metadata_filtering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26505f4f-b74b-4e64-a63e-88fc6e71ee98",
   "metadata": {},
   "source": [
    "## 1. Load Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5195aec4-6d01-4e2f-b5a8-134b20650ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '307297743176',\n",
       " 'regionName': 'us-west-2',\n",
       " 'collectionArn': 'arn:aws:aoss:us-west-2:307297743176:collection/h7cmj732p9d3v91spkhd',\n",
       " 'collectionId': 'h7cmj732p9d3v91spkhd',\n",
       " 'vectorIndexName': 'ws-index-',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::307297743176:role/advanced-rag-workshop-bedrock_execution_role-us-west-2',\n",
       " 's3Bucket': '307297743176-us-west-2-advanced-rag-workshop',\n",
       " 'kbFixedChunk': '4P6PBDDEGL',\n",
       " 'kbSemanticChunk': 'IC3ZCBORXT',\n",
       " 'kbCustomChunk': 'Q2T9CZ5VFA',\n",
       " 'kbHierarchicalChunk': '1YIFVW0Z5E',\n",
       " 'sagemakerLLMEndpoint': 'endpoint-llama-3-2-3b-instruct-2025-04-07-16-05-17',\n",
       " 'guardrail_id': 'fe7ryshi7i7b',\n",
       " 'guardrail_version': '1'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load configuration variables from a JSON file to access knowledge base ID, account number, and guardrail info.\n",
    "import json\n",
    "\n",
    "with open(\"../Lab 1/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables  # Display the loaded variables for confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d89542-cd78-4099-afcf-bcb0e29bec4e",
   "metadata": {},
   "source": [
    "## 2. Set Up Required IDs and Model ARNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d8cbc3-56ae-4aae-9a4f-b19c04693574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Base Selection  \n",
    "kb_id = variables[\"kbFixedChunk\"]  # Options: \"kbFixedChunk\", \"kbHierarchicalChunk\", \"kbSemanticChunk\"\n",
    "\n",
    "# Retrieval-Augmented Generation (RAG) Configuration  \n",
    "number_of_results = 3  # Number of relevant documents to retrieve  \n",
    "generation_configuration = {\n",
    "    \"temperature\": 0,  # Lower temperature for more deterministic responses  \n",
    "    \"top_k\": 10,  # Consider top 10 tokens at each generation step  \n",
    "    \"max_new_tokens\": 5000,  # Maximum number of tokens to generate  \n",
    "    \"stop\": \"<|eot_id|>\"  # Stop sequence to end the response generation  \n",
    "}\n",
    "\n",
    "# User Query\n",
    "query = \"what was the % increase in sales?\"  # Sample query to retrieve data from the knowledge base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8eb99-8dbf-4c1b-a6a5-a7ad9a559177",
   "metadata": {},
   "source": [
    "## 3. Define Metadata Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e43968-c317-4571-afd7-41abe8ee7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a metadata filter for advanced filtering based on specific conditions\n",
    "one_group_filter= {\n",
    "    \"andAll\": [\n",
    "        {\n",
    "            \"equals\": {\n",
    "                \"key\": \"docType\",\n",
    "                \"value\": '10K Report'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"equals\": {\n",
    "                \"key\": \"year\",\n",
    "                \"value\": 2023\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccdb8b-226d-47f8-be02-f8059e6d482a",
   "metadata": {},
   "source": [
    "## 4. Define SageMaker & Bedrock helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be4a1ea-6575-4a85-9b60-1c5f1524d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize Bedrock client to interact with the Bedrock Knowledge Base\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Constants for Knowledge Base ID, SageMaker endpoint, and number of results to retrieve\n",
    "KNOWLEDGE_BASE_ID = kb_id\n",
    "ENDPOINT_NAME = variables['sagemakerLLMEndpoint']\n",
    "NUM_RESULTS = number_of_results\n",
    "\n",
    "# Function to retrieve relevant context from the Bedrock Knowledge Base\n",
    "def retrieve_from_bedrock(query):\n",
    "    \"\"\"Retrieve relevant context from Bedrock Knowledge Base\"\"\"\n",
    "    try:\n",
    "        # Retrieve context based on the query using vector search configuration\n",
    "        response = bedrock_agent_runtime.retrieve(\n",
    "            knowledgeBaseId=KNOWLEDGE_BASE_ID,\n",
    "            retrievalQuery={\n",
    "                'text': query  # The query text to search in the knowledge base\n",
    "            },\n",
    "            retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': NUM_RESULTS,  # Adjust based on needs\n",
    "                     \"filter\": one_group_filter\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        # Extract the 'text' from the retrieval results and return as a list\n",
    "        return [result['content']['text'] for result in response['retrievalResults']]\n",
    "    except Exception as e:\n",
    "        # Raise an error if the retrieval process fails\n",
    "        raise RuntimeError(f\"Bedrock retrieval failed: {str(e)}\")\n",
    "\n",
    "# Function to format the prompt for Llama 3 model using retrieved context\n",
    "def format_prompt(query, context):\n",
    "    \"\"\"Format prompt for Llama 3\"\"\"\n",
    "    # Create the system prompt that includes the context and the user's question\n",
    "    system_prompt = f\"\"\"Use the following context to answer the question. If you don't know the answer, say 'I don't know'.\n",
    "        Context:\n",
    "        {\" \".join(context)}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the complete prompt including system and user instructions\n",
    "    return f\"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {system_prompt}\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Question: {query}\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\".strip()\n",
    "\n",
    "# Function to generate a response from the SageMaker endpoint based on the formatted prompt\n",
    "def generate_response(prompt):\n",
    "    \"\"\"Generate response using SageMaker endpoint\"\"\"\n",
    "    # Initialize SageMaker runtime client\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    # Prepare the payload with prompt and generation parameters\n",
    "    payload = {\n",
    "        \"inputs\": prompt,  # The formatted prompt to pass to the model\n",
    "        \"parameters\": generation_configuration  # Additional parameters for the model (e.g., temperature, tokens)\n",
    "    }\n",
    "    try:\n",
    "        # Call the SageMaker endpoint to generate the response\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=ENDPOINT_NAME,  # SageMaker endpoint name\n",
    "            ContentType='application/json',  # Content type for the request\n",
    "            Body=json.dumps(payload)  # Send the payload as JSON\n",
    "        )\n",
    "\n",
    "        # Parse the response body\n",
    "        result = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "        \n",
    "        # Handle different response formats (list or dictionary)\n",
    "        if isinstance(result, list):\n",
    "            # If the result is a list, extract the generated text from the first element\n",
    "            return result[0]['generated_text']\n",
    "        elif 'generated_text' in result:\n",
    "            # If the result is a dictionary with 'generated_text', return the generated text\n",
    "            return result['generated_text']\n",
    "        elif 'generation' in result:\n",
    "            # Alternative format with 'generation' key\n",
    "            return result['generation']\n",
    "        else:\n",
    "            # Raise an error if the response format is unexpected\n",
    "            raise RuntimeError(\"Unexpected response format\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Raise an error if the generation process fails\n",
    "        raise RuntimeError(f\"Generation failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a335d-e615-4885-823b-151d8a94c173",
   "metadata": {},
   "source": [
    "## 5. Generate Response with Metadata Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b60755c-9a5e-4d9a-998e-0006f79eae4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: {'what was the % increase in sales?'}\n",
      "Answer: \n",
      "\n",
      "According to the text, the sales increased 9% in 2022, compared to the prior year.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve relevant context from the Bedrock Knowledge Base based on the query\n",
    "context = retrieve_from_bedrock(query)\n",
    "\n",
    "# Format the prompt by combining the user's query and the retrieved context\n",
    "prompt = format_prompt(query, context)\n",
    "\n",
    "# Generate the response using the formatted prompt by calling the SageMaker endpoint\n",
    "response = generate_response(prompt)\n",
    "\n",
    "# Print the user's query\n",
    "print(\"Question:\", {query})\n",
    "\n",
    "# Uncomment below line if you want to debug and see the retrieved context\n",
    "# print(f\"Context: {context}\")\n",
    "\n",
    "# Print the generated answer from the model based on the query and context\n",
    "print(\"Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7eb89-845a-42f1-9135-f3bef68c8d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
